{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852d7da-7fe1-46bc-bf95-27ede4001bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!conda list\n",
    "!pip install opencv-python matplotlib\n",
    "!pip3 install torch torchvision torchaudio\n",
    "pip install opencv-python matplotlib\n",
    "!'git+https://github.com/facebookresearch/sam2.git'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030fa6d6-26c6-44be-bd39-eda6fcd200d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/sam2.git\n",
    "!cd sam2 && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df7f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall sam-2 -y\n",
    "!pip install git+https://github.com/facebookresearch/sam2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f607ec-2c1b-46cd-8e29-463e6c2c6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98924fa-7451-43e0-a572-fd9d1b7bc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac248d10-2004-4599-be28-f523213f6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from hydra import compose\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import sam2\n",
    "\n",
    "# Check if the user is running Python from the parent directory of the sam2 repo\n",
    "# (i.e. the directory where this repo is cloned into) -- this is not supported since\n",
    "# it could shadow the sam2 package and cause issues.\n",
    "if os.path.isdir(os.path.join(sam2.__path__[0], \"sam2\")):\n",
    "    # If the user has \"sam2/sam2\" in their path, they are likey importing the repo itself\n",
    "    # as \"sam2\" rather than importing the \"sam2\" python package (i.e. \"sam2/sam2\" directory).\n",
    "    # This typically happens because the user is running Python from the parent directory\n",
    "    # that contains the sam2 repo they cloned.\n",
    "    raise RuntimeError(\n",
    "        \"You're likely running Python from the parent directory of the sam2 repository \"\n",
    "        \"(i.e. the directory where https://github.com/facebookresearch/sam2 is cloned into). \"\n",
    "        \"This is not supported since the `sam2` Python package could be shadowed by the \"\n",
    "        \"repository name (the repository is also named `sam2` and contains the Python package \"\n",
    "        \"in `sam2/sam2`). Please run Python from another directory (e.g. from the repo dir \"\n",
    "        \"rather than its parent dir, or from your home directory) after installing SAM 2.\"\n",
    "    )\n",
    "\n",
    "\n",
    "HF_MODEL_ID_TO_FILENAMES = {\n",
    "    \"facebook/sam2-hiera-tiny\": (\n",
    "        \"configs/sam2/sam2_hiera_t.yaml\",\n",
    "        \"sam2_hiera_tiny.pt\",\n",
    "    ),\n",
    "    \"facebook/sam2-hiera-small\": (\n",
    "        \"configs/sam2/sam2_hiera_s.yaml\",\n",
    "        \"sam2_hiera_small.pt\",\n",
    "    ),\n",
    "    \"facebook/sam2-hiera-base-plus\": (\n",
    "        \"configs/sam2/sam2_hiera_b+.yaml\",\n",
    "        \"sam2_hiera_base_plus.pt\",\n",
    "    ),\n",
    "    \"facebook/sam2-hiera-large\": (\n",
    "        \"configs/sam2/sam2_hiera_l.yaml\",\n",
    "        \"sam2_hiera_large.pt\",\n",
    "    ),\n",
    "    \"facebook/sam2.1-hiera-tiny\": (\n",
    "        \"configs/sam2.1/sam2.1_hiera_t.yaml\",\n",
    "        \"sam2.1_hiera_tiny.pt\",\n",
    "    ),\n",
    "    \"facebook/sam2.1-hiera-small\": (\n",
    "        \"configs/sam2.1/sam2.1_hiera_s.yaml\",\n",
    "        \"sam2.1_hiera_small.pt\",\n",
    "    ),\n",
    "    \"facebook/sam2.1-hiera-base-plus\": (\n",
    "        \"configs/sam2.1/sam2.1_hiera_b+.yaml\",\n",
    "        \"sam2.1_hiera_base_plus.pt\",\n",
    "    ),\n",
    "    \"facebook/sam2.1-hiera-large\": (\n",
    "        \"configs/sam2.1/sam2.1_hiera_l.yaml\",\n",
    "        \"sam2.1_hiera_large.pt\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def build_sam2(\n",
    "    config_file,\n",
    "    ckpt_path=None,\n",
    "    device=\"mps\",\n",
    "    mode=\"eval\",\n",
    "    hydra_overrides_extra=[],\n",
    "    apply_postprocessing=True,\n",
    "    **kwargs,\n",
    "):\n",
    "\n",
    "    if apply_postprocessing:\n",
    "        hydra_overrides_extra = hydra_overrides_extra.copy()\n",
    "        hydra_overrides_extra += [\n",
    "            # dynamically fall back to multi-mask if the single mask is not stable\n",
    "            \"++model.sam_mask_decoder_extra_args.dynamic_multimask_via_stability=true\",\n",
    "            \"++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_delta=0.05\",\n",
    "            \"++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_thresh=0.98\",\n",
    "        ]\n",
    "    # Read config and init model\n",
    "    cfg = compose(config_name=config_file, overrides=hydra_overrides_extra)\n",
    "    OmegaConf.resolve(cfg)\n",
    "    model = instantiate(cfg.model, _recursive_=True)\n",
    "    _load_checkpoint(model, ckpt_path)\n",
    "    model = model.to(device)\n",
    "    if mode == \"eval\":\n",
    "        model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_sam2_video_predictor(\n",
    "    config_file,\n",
    "    ckpt_path=None,\n",
    "    device=\"mps\",\n",
    "    mode=\"eval\",\n",
    "    hydra_overrides_extra=[],\n",
    "    apply_postprocessing=True,\n",
    "    vos_optimized=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    hydra_overrides = [\n",
    "        \"++model._target_=sam2.sam2_video_predictor.SAM2VideoPredictor\",\n",
    "    ]\n",
    "    if vos_optimized:\n",
    "        hydra_overrides = [\n",
    "            \"++model._target_=sam2.sam2_video_predictor.SAM2VideoPredictorVOS\",\n",
    "            \"++model.compile_image_encoder=True\",  # Let sam2_base handle this\n",
    "        ]\n",
    "\n",
    "    if apply_postprocessing:\n",
    "        hydra_overrides_extra = hydra_overrides_extra.copy()\n",
    "        hydra_overrides_extra += [\n",
    "            # dynamically fall back to multi-mask if the single mask is not stable\n",
    "            \"++model.sam_mask_decoder_extra_args.dynamic_multimask_via_stability=true\",\n",
    "            \"++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_delta=0.05\",\n",
    "            \"++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_thresh=0.98\",\n",
    "            # the sigmoid mask logits on interacted frames with clicks in the memory encoder so that the encoded masks are exactly as what users see from clicking\n",
    "            \"++model.binarize_mask_from_pts_for_mem_enc=true\",\n",
    "            # fill small holes in the low-res masks up to `fill_hole_area` (before resizing them to the original video resolution)\n",
    "            \"++model.fill_hole_area=8\",\n",
    "        ]\n",
    "    hydra_overrides.extend(hydra_overrides_extra)\n",
    "\n",
    "    # Read config and init model\n",
    "    cfg = compose(config_name=config_file, overrides=hydra_overrides)\n",
    "    OmegaConf.resolve(cfg)\n",
    "    model = instantiate(cfg.model, _recursive_=True)\n",
    "    _load_checkpoint(model, ckpt_path)\n",
    "    model = model.to(device)\n",
    "    if mode == \"eval\":\n",
    "        model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def _hf_download(model_id):\n",
    "    from huggingface_hub import hf_hub_download\n",
    "\n",
    "    config_name, checkpoint_name = HF_MODEL_ID_TO_FILENAMES[model_id]\n",
    "    ckpt_path = hf_hub_download(repo_id=model_id, filename=checkpoint_name)\n",
    "    return config_name, ckpt_path\n",
    "\n",
    "\n",
    "def build_sam2_hf(model_id, **kwargs):\n",
    "    config_name, ckpt_path = _hf_download(model_id)\n",
    "    return build_sam2(config_file=config_name, ckpt_path=ckpt_path, **kwargs)\n",
    "\n",
    "\n",
    "def build_sam2_video_predictor_hf(model_id, **kwargs):\n",
    "    config_name, ckpt_path = _hf_download(model_id)\n",
    "    return build_sam2_video_predictor(\n",
    "        config_file=config_name, ckpt_path=ckpt_path, **kwargs\n",
    "    )\n",
    "\n",
    "\n",
    "def _load_checkpoint(model, ckpt_path):\n",
    "    if ckpt_path is not None:\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(sd)\n",
    "        if missing_keys:\n",
    "            logging.error(missing_keys)\n",
    "            raise RuntimeError()\n",
    "        if unexpected_keys:\n",
    "            logging.error(unexpected_keys)\n",
    "            raise RuntimeError()\n",
    "        logging.info(\"Loaded checkpoint sucessfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d962c4e-c05f-43b8-929a-5d3c9ce7c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"facebook/sam2.1-hiera-large\"\n",
    "predictor = build_sam2_video_predictor_hf(model_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf719fa1-9b2e-4524-8db8-45c504d051f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211b853-0013-4969-a919-c9cd3135753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "video_dir = \"./ceramics_frames\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db95a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52726cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c497a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a positive click at (x, y) = (210, 350) to get started\n",
    "points = np.array([[210, 350]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1], np.int32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5929139",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd positive click at (x, y) = (250, 220) to refine the mask\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[210, 350], [250, 220]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 1], np.int32)\n",
    "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ca0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 150  # further refine some details on this frame\n",
    "ann_obj_id = 1  # give a unique id to the object we interact with (it can be any integers)\n",
    "\n",
    "# show the segment before further refinement\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- before refinement\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_mask(video_segments[ann_frame_idx][ann_obj_id], plt.gca(), obj_id=ann_obj_id)\n",
    "\n",
    "# Let's add a negative click on this frame at (x, y) = (82, 415) to refine the segment\n",
    "points = np.array([[82, 410]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([0], np.int32)\n",
    "_, _, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "# show the segment after the further refinement\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx} -- after refinement\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "show_mask((out_mask_logits > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
