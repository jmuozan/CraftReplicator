{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd7c4f5a-ea67-4177-a10b-7b771f99f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edee44f-d2ec-445c-82fe-4ec3556989f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.5.1\n",
      "device:  mps\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version: \", torch.__version__)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5df1af9-8d1c-468c-b03e-cc688b354f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (100, 400, 400, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 60/60 [31:10<00:00, 31.17s/it]\n",
      "100%|███████████████████████████████████████████| 60/60 [31:10<00:00, 31.18s/it]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, camera_intrinsics, camera_extrinsics, hn, hf, images, chunk_size=10, img_index=0, nb_bins=192, H=400,\n",
    "         W=400):\n",
    "    ray_origins, ray_directions, _ = sample_batch(camera_extrinsics, camera_intrinsics, images, None, H, W,\n",
    "                                                  img_index=img_index, sample_all=True)\n",
    "    data = []\n",
    "    for i in range(int(np.ceil(H / chunk_size))):\n",
    "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(camera_intrinsics.device)\n",
    "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(camera_intrinsics.device)\n",
    "        regenerated_px_values = render_rays(model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "        data.append(regenerated_px_values)\n",
    "    img = torch.cat(data).data.cpu().numpy().reshape(H, W, 3)\n",
    "    plt.imshow(img)\n",
    "    plt.savefig(f'Imgs/novel_view.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "class NerfModel(nn.Module):\n",
    "    def __init__(self, embedding_dim_pos=10, embedding_dim_direction=4, hidden_dim=128):\n",
    "        super(NerfModel, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), )\n",
    "        self.block2 = nn.Sequential(nn.Linear(embedding_dim_pos * 6 + hidden_dim + 3, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "                                    nn.Linear(hidden_dim, hidden_dim + 1), )\n",
    "        self.block3 = nn.Sequential(nn.Linear(embedding_dim_direction * 6 + hidden_dim + 3, hidden_dim // 2),\n",
    "                                    nn.ReLU(), )\n",
    "        self.block4 = nn.Sequential(nn.Linear(hidden_dim // 2, 3), nn.Sigmoid(), )\n",
    "\n",
    "        self.embedding_dim_pos = embedding_dim_pos\n",
    "        self.embedding_dim_direction = embedding_dim_direction\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    @staticmethod\n",
    "    def positional_encoding(x, L):\n",
    "        out = [x]\n",
    "        for j in range(L):\n",
    "            out.append(torch.sin(2 ** j * x))\n",
    "            out.append(torch.cos(2 ** j * x))\n",
    "        return torch.cat(out, dim=1)\n",
    "\n",
    "    def forward(self, o, d):\n",
    "        emb_x = self.positional_encoding(o, self.embedding_dim_pos)\n",
    "        emb_d = self.positional_encoding(d, self.embedding_dim_direction)\n",
    "        tmp = self.block2(torch.cat((self.block1(emb_x), emb_x), dim=1))\n",
    "        h, sigma = tmp[:, :-1], self.relu(tmp[:, -1])\n",
    "        c = self.block4(self.block3(torch.cat((h, emb_d), dim=1)))\n",
    "        return c, sigma\n",
    "\n",
    "\n",
    "def compute_accumulated_transmittance(alphas):\n",
    "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
    "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
    "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
    "\n",
    "\n",
    "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
    "    device = ray_origins.device\n",
    "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
    "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
    "    lower = torch.cat((t[:, :1], mid), -1)\n",
    "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
    "    u = torch.rand(t.shape, device=device)  # Perturb sampling along each ray.\n",
    "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
    "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
    "\n",
    "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)  # [batch_size, nb_bins, 3]\n",
    "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
    "\n",
    "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
    "    alpha = 1 - torch.exp(-sigma.reshape(x.shape[:-1]) * delta)  # [batch_size, nb_bins]\n",
    "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
    "    return (weights * colors.reshape(x.shape)).sum(dim=1)  # Pixel values\n",
    "\n",
    "\n",
    "def train(nerf_model, optimizers, schedulers, training_images, camera_extrinsics, camera_intrinsics, batch_size,\n",
    "          nb_epochs, hn=0., hf=1., nb_bins=192):\n",
    "    H, W = training_images.shape[1:3]\n",
    "\n",
    "    training_loss = []\n",
    "    for _ in tqdm(range(nb_epochs)):\n",
    "        ids = np.arange(training_images.shape[0])\n",
    "        np.random.shuffle(ids)\n",
    "        for img_index in ids:\n",
    "            rays_o, rays_d, samples_idx = sample_batch(camera_extrinsics, camera_intrinsics, training_images,\n",
    "                                                       batch_size, H, W, img_index=img_index)\n",
    "            gt_px_values = torch.from_numpy(training_images[samples_idx]).to(camera_intrinsics.device)\n",
    "            regenerated_px_values = render_rays(nerf_model, rays_o, rays_d, hn=hn, hf=hf, nb_bins=nb_bins)\n",
    "            loss = ((gt_px_values - regenerated_px_values) ** 2).sum()\n",
    "\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            for optimizer in optimizers:\n",
    "                optimizer.step()\n",
    "            training_loss.append(loss.item())\n",
    "        for scheduler in schedulers:\n",
    "            scheduler.step()\n",
    "    return training_loss\n",
    "\n",
    "\n",
    "def initialize_camera_parameters(images, device=device):\n",
    "    camera_intrinsics = torch.ones(1, device=device, requires_grad=True)\n",
    "    camera_extrinsics = torch.zeros((images.shape[0], 6), device=device, dtype=torch.float32, requires_grad=True)\n",
    "    return camera_intrinsics, camera_extrinsics\n",
    "\n",
    "\n",
    "def load_images(data_path):\n",
    "    image_paths = glob.glob(data_path)\n",
    "    images = None\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        img = imread(image_path)\n",
    "        # Ensure we only take RGB channels if image is RGBA\n",
    "        if img.shape[-1] == 4:\n",
    "            img = img[..., :3]\n",
    "        # Add batch dimension\n",
    "        img = np.expand_dims(img, 0)\n",
    "        images = np.concatenate((images, img)) if images is not None else img\n",
    "    return images\n",
    "\n",
    "\n",
    "def get_ndc_rays(H, W, focal, rays_o, rays_d, near=1.):\n",
    "    # We shift o to the ray’s intersection with the near plane at z = −n (before the NDC conversion)\n",
    "    t = -(near + rays_o[..., 2]) / rays_d[..., 2]\n",
    "    rays_o = rays_o + t[..., None] * rays_d\n",
    "\n",
    "    rays_o = torch.stack([- focal / W / 2. * rays_o[..., 0] / rays_o[..., 2],\n",
    "                          - focal / H / 2. * rays_o[..., 1] / rays_o[..., 2],\n",
    "                          1. + 2. * near / rays_o[..., 2]], -1)  # Eq 25 https://arxiv.org/pdf/2003.08934.pdf\n",
    "    rays_d = torch.stack([- focal / W / 2. * (rays_d[..., 0] / rays_d[..., 2] - rays_o[..., 0] / rays_o[..., 2]),\n",
    "                          - focal / H / 2. * (rays_d[..., 1] / rays_d[..., 2] - rays_o[..., 1] / rays_o[..., 2]),\n",
    "                          - 2. * near / rays_o[..., 2]], -1)  # Eq 26 https://arxiv.org/pdf/2003.08934.pdf\n",
    "    return rays_o, rays_d\n",
    "\n",
    "\n",
    "def sample_batch(camera_extrinsics, camera_intrinsics, images, batch_size, H, W, img_index=0, sample_all=False):\n",
    "    if sample_all:\n",
    "        image_indices = (torch.zeros(W * H) + img_index).type(torch.long)\n",
    "        u, v = np.meshgrid(np.linspace(0, W - 1, W, dtype=int), np.linspace(0, H - 1, H, dtype=int))\n",
    "        u = torch.from_numpy(u.reshape(-1)).to(camera_intrinsics.device)\n",
    "        v = torch.from_numpy(v.reshape(-1)).to(camera_intrinsics.device)\n",
    "    else:\n",
    "        image_indices = (torch.zeros(batch_size) + img_index).type(torch.long)  # Sample random images\n",
    "        u = torch.randint(W, (batch_size,), device=camera_intrinsics.device)  # Sample random pixels\n",
    "        v = torch.randint(H, (batch_size,), device=camera_intrinsics.device)\n",
    "\n",
    "    focal = camera_intrinsics[0] ** 2 * W\n",
    "    t = camera_extrinsics[img_index, :3]\n",
    "    r = camera_extrinsics[img_index, -3:]\n",
    "\n",
    "    # Creating the c2w matrix, Section 4.1 from the paper\n",
    "    phi_skew = torch.stack([torch.cat([torch.zeros(1, device=r.device), -r[2:3], r[1:2]]),\n",
    "                            torch.cat([r[2:3], torch.zeros(1, device=r.device), -r[0:1]]),\n",
    "                            torch.cat([-r[1:2], r[0:1], torch.zeros(1, device=r.device)])], dim=0)\n",
    "    alpha = r.norm() + 1e-15\n",
    "    R = torch.eye(3, device=r.device) + (torch.sin(alpha) / alpha) * phi_skew + (\n",
    "            (1 - torch.cos(alpha)) / alpha ** 2) * (phi_skew @ phi_skew)\n",
    "    c2w = torch.cat([R, t.unsqueeze(1)], dim=1)\n",
    "    c2w = torch.cat([c2w, torch.tensor([[0., 0., 0., 1.]], device=c2w.device)], dim=0)\n",
    "\n",
    "    rays_d_cam = torch.cat([((u.to(camera_intrinsics.device) - .5 * W) / focal).unsqueeze(-1),\n",
    "                            (-(v.to(camera_intrinsics.device) - .5 * H) / focal).unsqueeze(-1),\n",
    "                            - torch.ones_like(u).unsqueeze(-1)], dim=-1)\n",
    "    rays_d_world = torch.matmul(c2w[:3, :3].view(1, 3, 3), rays_d_cam.unsqueeze(2)).squeeze(2)\n",
    "    rays_o_world = c2w[:3, 3].view(1, 3).expand_as(rays_d_world)\n",
    "    rays_o_world, rays_d_world = get_ndc_rays(H, W, focal, rays_o=rays_o_world, rays_d=rays_d_world)\n",
    "    return rays_o_world, F.normalize(rays_d_world, p=2, dim=1), (image_indices, v.cpu(), u.cpu())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = device\n",
    "    nb_epochs = 60 #int(1e4)\n",
    "\n",
    "    training_images = load_images(\"fox/_fox/imgs/*.png\")\n",
    "    print(\"Training images shape:\", training_images.shape)\n",
    "    camera_intrinsics, camera_extrinsics = initialize_camera_parameters(training_images, device=device)\n",
    "    batch_size = 1024\n",
    "    import os\n",
    "\n",
    "    # Create Imgs directory if it doesn't exist\n",
    "    os.makedirs('Imgs', exist_ok=True)\n",
    "\n",
    "    # Part 1\n",
    "    model = NerfModel(hidden_dim=256).to(device)\n",
    "    model_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    optimizer_camera_parameters = torch.optim.Adam({camera_extrinsics}, lr=0.0009)\n",
    "    optimizer_focal = torch.optim.Adam({camera_intrinsics}, lr=0.001)\n",
    "    scheduler_model = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        model_optimizer, [10 * (i + 1) for i in range(nb_epochs // 10)], gamma=0.9954)\n",
    "    scheduler_camera_parameters = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer_camera_parameters, [100 * (i + 1) for i in range(nb_epochs // 100)], gamma=0.81)\n",
    "    scheduler_focal = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer_focal, [100 * (i + 1) for i in range(nb_epochs // 100)], gamma=0.9)\n",
    "    train(model, [model_optimizer, optimizer_camera_parameters, optimizer_focal],\n",
    "          [scheduler_model, scheduler_camera_parameters, scheduler_focal], training_images, camera_extrinsics,\n",
    "          camera_intrinsics, batch_size, nb_epochs, hn=0., hf=1., nb_bins=192)\n",
    "\n",
    "    # Part 2\n",
    "    model = NerfModel(hidden_dim=256).to(device)\n",
    "    model_optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler_model = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        model_optimizer, [10 * (i + 1) for i in range(nb_epochs // 10)], gamma=0.9954)\n",
    "    train(model, [model_optimizer], [scheduler_model], training_images, camera_extrinsics, camera_intrinsics,\n",
    "          batch_size, nb_epochs, hn=0., hf=1., nb_bins=192)\n",
    "\n",
    "    # Test: interpolation between two images\n",
    "    test(model, camera_intrinsics, (.5 * camera_extrinsics[0] + .5 * camera_extrinsics[1]).unsqueeze(0), 0., 1.,\n",
    "         training_images, img_index=0, nb_bins=192, H=training_images.shape[1], W=training_images.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ad62e-4f8a-436d-9268-dd457bd2374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c67b8-5ee8-41e5-abb1-4abc2de34efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5922e325-8976-4a36-88aa-908cbe4d3953",
   "metadata": {},
   "source": [
    "# Camera / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e62ab8-9524-48b6-a646-01b27c9a3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "o, d, target_px_values = get_rays('fox', mode='train')\n",
    "dataloader = DataLoader(torch.cat((torch.from_numpy(o).reshape(-1, 3).type(torch.float),\n",
    "                                   torch.from_numpy(d).reshape(-1, 3).type(torch.float),\n",
    "                                   torch.from_numpy(target_px_values).reshape(-1, 3).type(torch.float)), dim=1),\n",
    "                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "dataloader_warmup = DataLoader(torch.cat((torch.from_numpy(o).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(d).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(target_px_values).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float)), dim=1),\n",
    "                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_o, test_d, test_target_px_values = get_rays('fox', mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc6284-ff38-4a9d-a914-7cd8ff9163fb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd96eec-e258-469d-8781-da1dc2bd1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device\n",
    "\n",
    "tn = 8.\n",
    "tf = 12.\n",
    "nb_epochs = 1 #15 30\n",
    "lr =  1e-3 # 1e-3 5e-4\n",
    "gamma = .5 #0.5 0.7 \n",
    "nb_bins = 100 #100 256\n",
    "\n",
    "model = Nerf(hidden_dim=256).to(device) #Nerf(hidden_dim=128).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10], gamma=gamma)\n",
    "\n",
    "\n",
    "\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, 1, dataloader_warmup, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, nb_epochs, dataloader, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b2ceae-86b6-4d6d-aafe-ac3627807f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_nerf_colmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be732b56-fa20-4a47-8b2a-f918c1ea5d7a",
   "metadata": {},
   "source": [
    "# Mesh extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6764d29-c8b9-417f-ad7f-a2aa42554303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/_x9hjmys03x5gnbfl70ry2sr0000gn/T/ipykernel_38630/2503494064.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('model_nerf_colmap').to(device)\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('model_nerf_colmap').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74e0f3d9-71c0-4149-93a3-a16ff6263e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating density volume with resolution 700...\n",
      "Sampling density field...\n",
      "Density field statistics:\n",
      "Min: 0.000000\n",
      "Max: 0.000000\n",
      "Mean: 0.000000\n",
      "Std: 0.000000\n",
      "Auto-determined threshold: 0.000000\n",
      "Extracting mesh with threshold 0.0...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No surface found at the given iso value.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnerf_mesh.obj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Can also use .obj format\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Extract and save the mesh\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m mesh \u001b[38;5;241m=\u001b[39m \u001b[43msave_colored_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 121\u001b[0m, in \u001b[0;36msave_colored_mesh\u001b[0;34m(nerf_model, output_path, resolution, threshold, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_colored_mesh\u001b[39m(nerf_model, output_path, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Extract and save a colored mesh from a NeRF model.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m        device: Torch device to use\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m \u001b[43mextract_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnerf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing mesh...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# Optional mesh cleanup\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 70\u001b[0m, in \u001b[0;36mextract_mesh\u001b[0;34m(nerf_model, resolution, threshold, bbox_min, bbox_max, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting mesh with threshold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# Extract mesh using marching cubes\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     vertices, faces, normals, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmarching_cubes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdensity_volume\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspacing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_max\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_max\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_max\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbbox_min\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during marching cubes:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/newenv/lib/python3.9/site-packages/skimage/measure/_marching_cubes_lewiner.py:139\u001b[0m, in \u001b[0;36mmarching_cubes\u001b[0;34m(volume, level, spacing, gradient_direction, step_size, allow_degenerate, method, mask)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlewiner\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod should be either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlewiner\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlorensen\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_marching_cubes_lewiner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspacing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_direction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_degenerate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_classic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_classic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/newenv/lib/python3.9/site-packages/skimage/measure/_marching_cubes_lewiner.py:206\u001b[0m, in \u001b[0;36m_marching_cubes_lewiner\u001b[0;34m(volume, level, spacing, gradient_direction, step_size, allow_degenerate, use_classic, mask)\u001b[0m\n\u001b[1;32m    201\u001b[0m vertices, faces, normals, values \u001b[38;5;241m=\u001b[39m func(\n\u001b[1;32m    202\u001b[0m     volume, level, L, step_size, use_classic, mask\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vertices):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo surface found at the given iso value.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Output in z-y-x order, as is common in skimage\u001b[39;00m\n\u001b[1;32m    209\u001b[0m vertices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfliplr(vertices)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No surface found at the given iso value."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_density_field(density_volume):\n",
    "    \"\"\"Analyze the density field to help choose a good threshold.\"\"\"\n",
    "    min_density = float(density_volume.min())\n",
    "    max_density = float(density_volume.max())\n",
    "    mean_density = float(density_volume.mean())\n",
    "    std_density = float(density_volume.std())\n",
    "    \n",
    "    print(f\"Density field statistics:\")\n",
    "    print(f\"Min: {min_density:.6f}\")\n",
    "    print(f\"Max: {max_density:.6f}\")\n",
    "    print(f\"Mean: {mean_density:.6f}\")\n",
    "    print(f\"Std: {std_density:.6f}\")\n",
    "    \n",
    "    # Suggest threshold as mean + 1 std deviation\n",
    "    suggested_threshold = mean_density + std_density\n",
    "    return suggested_threshold\n",
    "\n",
    "def extract_mesh(nerf_model, resolution=128, threshold=None, bbox_min=[-1.5, -1.5, -1.5], \n",
    "                bbox_max=[1.5, 1.5, 1.5], device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Extract a colored mesh from a trained NeRF model.\n",
    "    \n",
    "    Args:\n",
    "        nerf_model: Trained NeRF model\n",
    "        resolution: Grid resolution for marching cubes\n",
    "        threshold: Density threshold for surface extraction (if None, will be auto-determined)\n",
    "        bbox_min: Minimum corner of bounding box\n",
    "        bbox_max: Maximum corner of bounding box\n",
    "        device: Torch device to use\n",
    "    \n",
    "    Returns:\n",
    "        trimesh.Trimesh: Colored mesh\n",
    "    \"\"\"\n",
    "    print(f\"Creating density volume with resolution {resolution}...\")\n",
    "    \n",
    "    # Create grid of points\n",
    "    x = torch.linspace(bbox_min[0], bbox_max[0], resolution)\n",
    "    y = torch.linspace(bbox_min[1], bbox_max[1], resolution)\n",
    "    z = torch.linspace(bbox_min[2], bbox_max[2], resolution)\n",
    "    xx, yy, zz = torch.meshgrid(x, y, z, indexing='ij')\n",
    "    points = torch.stack([xx, yy, zz], dim=-1).to(device)\n",
    "    \n",
    "    # Create density volume\n",
    "    density_volume = torch.zeros((resolution, resolution, resolution))\n",
    "    chunk_size = 512 * 512  # Process in chunks to avoid OOM\n",
    "    \n",
    "    print(\"Sampling density field...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, points.numel() // 3, chunk_size):\n",
    "            chunk_points = points.reshape(-1, 3)[i:i+chunk_size]\n",
    "            # Assume model returns (rgb, sigma) tuple\n",
    "            _, chunk_densities = nerf_model(chunk_points, torch.zeros_like(chunk_points))\n",
    "            density_volume.reshape(-1)[i:i+chunk_size] = chunk_densities.cpu()\n",
    "    \n",
    "    # Auto-determine threshold if not provided\n",
    "    if threshold is None:\n",
    "        threshold = analyze_density_field(density_volume)\n",
    "        print(f\"Auto-determined threshold: {threshold:.6f}\")\n",
    "    \n",
    "    print(f\"Extracting mesh with threshold {threshold}...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract mesh using marching cubes\n",
    "        vertices, faces, normals, _ = marching_cubes(\n",
    "            density_volume.numpy(),\n",
    "            threshold,\n",
    "            spacing=((bbox_max[0] - bbox_min[0])/resolution,\n",
    "                    (bbox_max[1] - bbox_min[1])/resolution,\n",
    "                    (bbox_max[2] - bbox_min[2])/resolution)\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\"Error during marching cubes:\")\n",
    "        print(e)\n",
    "        print(\"\\nTry adjusting the threshold based on the density statistics above.\")\n",
    "        raise\n",
    "    \n",
    "    print(f\"Mesh extracted with {len(vertices)} vertices and {len(faces)} faces\")\n",
    "    \n",
    "    # Adjust vertices to match bbox\n",
    "    vertices = vertices + np.array(bbox_min)\n",
    "    \n",
    "    # Sample colors at vertex positions\n",
    "    vertex_colors = torch.zeros((len(vertices), 3))\n",
    "    vertices_tensor = torch.tensor(vertices, dtype=torch.float32).to(device)\n",
    "    \n",
    "    print(\"Sampling colors...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(vertices), chunk_size):\n",
    "            chunk_vertices = vertices_tensor[i:i+chunk_size]\n",
    "            # Assume model returns (rgb, sigma) tuple\n",
    "            chunk_colors, _ = nerf_model(chunk_vertices, torch.zeros_like(chunk_vertices))\n",
    "            vertex_colors[i:i+chunk_size] = chunk_colors.cpu()\n",
    "    \n",
    "    # Create mesh with vertex colors\n",
    "    mesh = trimesh.Trimesh(\n",
    "        vertices=vertices,\n",
    "        faces=faces,\n",
    "        vertex_colors=(vertex_colors.numpy() * 255).astype(np.uint8),\n",
    "        vertex_normals=normals\n",
    "    )\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "def save_colored_mesh(nerf_model, output_path, resolution=256, threshold=None, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Extract and save a colored mesh from a NeRF model.\n",
    "    \n",
    "    Args:\n",
    "        nerf_model: Trained NeRF model\n",
    "        output_path: Path to save the mesh (should end in .ply or .obj)\n",
    "        resolution: Resolution for marching cubes\n",
    "        threshold: Density threshold (if None, will be auto-determined)\n",
    "        device: Torch device to use\n",
    "    \"\"\"\n",
    "    mesh = extract_mesh(nerf_model, resolution=resolution, threshold=threshold, device=device)\n",
    "    \n",
    "    print(\"Processing mesh...\")\n",
    "    # Optional mesh cleanup\n",
    "    mesh = mesh.process(validate=True)\n",
    "    \n",
    "    print(f\"Saving mesh to {output_path}...\")\n",
    "    # Save the mesh\n",
    "    mesh.export(output_path)\n",
    "    return mesh\n",
    "\n",
    "# After loading your model\n",
    "resolution = 700  # Increase for better quality, decrease if you run into memory issues\n",
    "output_path = \"nerf_mesh.obj\"  # Can also use .obj format\n",
    "\n",
    "# Extract and save the mesh\n",
    "mesh = save_colored_mesh(model, output_path, resolution=resolution, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b528771-b4d7-4285-8fd3-d15842b3f8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e15e1-1f6b-4372-9ee0-22a25cea75c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d3a25-9d9d-4248-a210-9876f24a5f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2956aa-8e87-44fc-94d9-8b3fd4c54569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91e1c2-ce41-4977-a3f9-944067a3af11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b0840-5978-4b21-b1b0-07dc04daa80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99cb50-6a3a-482f-9e91-493359805a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ece44-7ef1-4086-ac35-0054a0d37394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Pymcubes\n",
    "#!pip install trimesh\n",
    "#!pip install -U scikit-image\n",
    "#!pip install genesis-world  # Requires Python >=3.9;\n",
    "#!pip uninstall genesis-world\n",
    "#!conda install -c anaconda trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1340391-1311-4703-87db-6b37b442b4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac8b37-99bb-41b7-92e3-71ef9d9f9e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c97ed5-c343-4e17-957a-137d1c1a1acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de10ac4-4564-416c-81f5-0c6d22286d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newenv] *",
   "language": "python",
   "name": "conda-env-newenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
