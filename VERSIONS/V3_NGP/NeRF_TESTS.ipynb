{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W8v6j-22yrx"
      },
      "source": [
        "# Recreating NeRF\n",
        "This is a PyTorch implementation based on the paper: https://arxiv.org/abs/2003.08934. The code takes mostly after the officially tiny nerf implementation: https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install numpy tqdm ipywidgets matplotlib\n",
        "!pip3 install torch torchvision torchaudio\n",
        "!pip install imageio\n",
        "!pip install python-ffmpeg\n",
        "!pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PaCftwbyPpZ"
      },
      "source": [
        "## Imports + Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM86Hc4wM2db"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.init as init\n",
        "import time\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from ipywidgets import interactive, widgets\n",
        "\n",
        "#Example data http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyZgyFadyW5u"
      },
      "source": [
        "## Encoding + Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Wye6qB5uA8"
      },
      "source": [
        "$$\\gamma(p) = (\\sin(2^0\\pi p), \\cos(2^0\\pi p), \\dots, \\sin(2^{L-1}\\pi p), \\cos(2^{L-1}\\pi p))$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA7TIco3yWZP"
      },
      "outputs": [],
      "source": [
        "def encoding(x, L=10):\n",
        "    rets = [x]\n",
        "    for i in range(L):\n",
        "        for fn in [torch.sin, torch.cos]:  # Use torch functions\n",
        "            rets.append(fn(2. ** i * x))\n",
        "    return torch.cat(rets, dim=-1)  # Concatenate along the last dimension\n",
        "\n",
        "class NeRF(nn.Module):\n",
        "\n",
        "  def __init__(self, pos_enc_dim = 63, view_enc_dim = 27, hidden = 256) -> None:\n",
        "     super().__init__()\n",
        "\n",
        "     self.linear1 = nn.Sequential(nn.Linear(pos_enc_dim,hidden),nn.ReLU())\n",
        "\n",
        "     self.pre_skip_linear = nn.Sequential()\n",
        "     for _ in range(4):\n",
        "      self.pre_skip_linear.append(nn.Linear(hidden,hidden))\n",
        "      self.pre_skip_linear.append(nn.ReLU())\n",
        "\n",
        "\n",
        "     self.linear_skip = nn.Sequential(nn.Linear(hidden+pos_enc_dim,hidden),nn.ReLU())\n",
        "\n",
        "     self.post_skip_linear = nn.Sequential()\n",
        "     for _ in range(2):\n",
        "      self.post_skip_linear.append(nn.Linear(hidden,hidden))\n",
        "      self.post_skip_linear.append(nn.ReLU())\n",
        "\n",
        "     self.density_layer = nn.Sequential(nn.Linear(hidden,1), nn.ReLU())\n",
        "\n",
        "     self.linear2 = nn.Linear(hidden,hidden)\n",
        "\n",
        "     self.color_linear1 = nn.Sequential(nn.Linear(hidden+view_enc_dim,hidden//2),nn.ReLU())\n",
        "     self.color_linear2 = nn.Sequential(nn.Linear(hidden//2, 3),nn.Sigmoid())\n",
        "\n",
        "     self.relu = nn.ReLU()\n",
        "     self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self,input):\n",
        "\n",
        "    # Extract pos and view dirs\n",
        "    positions = input[..., :3]\n",
        "    view_dirs = input[...,3:]\n",
        "\n",
        "    # Encode\n",
        "    pos_enc = encoding(positions,L=10)\n",
        "    view_enc = encoding(view_dirs, L=4)\n",
        "\n",
        "    x = self.linear1(pos_enc)\n",
        "    x = self.pre_skip_linear(x)\n",
        "\n",
        "    # Skip connection\n",
        "    x = torch.cat([x, pos_enc], dim=-1)\n",
        "    x = self.linear_skip(x)\n",
        "\n",
        "    x = self.post_skip_linear(x)\n",
        "\n",
        "    # Density prediction\n",
        "    sigma = self.density_layer(x)\n",
        "\n",
        "    x = self.linear2(x)\n",
        "\n",
        "    # Incoroporate view encoding\n",
        "    x = torch.cat([x, view_enc],dim=-1)\n",
        "    x = self.color_linear1(x)\n",
        "\n",
        "    # Color Prediction\n",
        "    rgb = self.color_linear2(x)\n",
        "\n",
        "    return torch.cat([sigma, rgb], dim=-1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vra9EvhaydjT"
      },
      "source": [
        "## Get Ray + Render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g6UAPBoyjhB"
      },
      "outputs": [],
      "source": [
        "def get_rays(H, W, focal, c2w):\n",
        "    \"\"\"\n",
        "    Generate rays for a given camera configuration.\n",
        "\n",
        "    Args:\n",
        "      H: Image height.\n",
        "      W: Image width.\n",
        "      focal: Focal length.\n",
        "      c2w: Camera-to-world transformation matrix (4x4).\n",
        "\n",
        "    Returns:\n",
        "      rays_o: Ray origins (H*W, 3).\n",
        "      rays_d: Ray directions (H*W, 3).\n",
        "    \"\"\"\n",
        "    device = c2w.device  # Get the device of c2w\n",
        "    \n",
        "    # Convert focal to float32 before moving to device\n",
        "    if isinstance(focal, np.ndarray):\n",
        "        focal = torch.tensor(focal.item(), dtype=torch.float32, device=device)\n",
        "    elif not isinstance(focal, torch.Tensor):\n",
        "        focal = torch.tensor(focal, dtype=torch.float32, device=device)\n",
        "    elif focal.device != device:\n",
        "        focal = focal.to(device)\n",
        "\n",
        "    i, j = torch.meshgrid(\n",
        "        torch.arange(W, dtype=torch.float32, device=device),\n",
        "        torch.arange(H, dtype=torch.float32, device=device),\n",
        "        indexing='xy'\n",
        "    )\n",
        "    \n",
        "    # Process in chunks if needed for extremely high resolutions\n",
        "    dirs = torch.stack(\n",
        "        [(i - W * 0.5) / focal, -(j - H * 0.5) / focal, -torch.ones_like(i, device=device)], -1\n",
        "    )\n",
        "    \n",
        "    # Matrix multiply in smaller batches if needed\n",
        "    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
        "    rays_d = rays_d.reshape(-1, 3)\n",
        "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
        "    \n",
        "    return rays_o, rays_d\n",
        "\n",
        "def get_rays_optimized(H, W, focal, c2w, downscale_factor=1):\n",
        "    \"\"\"\n",
        "    Generate rays for a given camera configuration with option to downscale.\n",
        "    \"\"\"\n",
        "    device = c2w.device\n",
        "    \n",
        "    # Downscale for memory efficiency\n",
        "    h = H // downscale_factor\n",
        "    w = W // downscale_factor\n",
        "    focal_scaled = focal / downscale_factor\n",
        "    \n",
        "    # Convert focal to float32 before moving to device\n",
        "    if isinstance(focal_scaled, np.ndarray):\n",
        "        focal_scaled = torch.tensor(focal_scaled.item(), dtype=torch.float32, device=device)\n",
        "    elif not isinstance(focal_scaled, torch.Tensor):\n",
        "        focal_scaled = torch.tensor(focal_scaled, dtype=torch.float32, device=device)\n",
        "    elif focal_scaled.device != device:\n",
        "        focal_scaled = focal_scaled.to(device)\n",
        "    \n",
        "    # Use reduced resolution grid\n",
        "    i, j = torch.meshgrid(\n",
        "        torch.arange(w, dtype=torch.float32, device=device),\n",
        "        torch.arange(h, dtype=torch.float32, device=device),\n",
        "        indexing='xy'\n",
        "    )\n",
        "    \n",
        "    dirs = torch.stack(\n",
        "        [(i - w * 0.5) / focal_scaled, -(j - h * 0.5) / focal_scaled, -torch.ones_like(i, device=device)], -1\n",
        "    )\n",
        "    \n",
        "    # Matrix multiply in smaller batches to save memory\n",
        "    rays_d = torch.sum(dirs[..., None, :] * c2w[:3, :3], -1)\n",
        "    rays_d = rays_d.reshape(-1, 3)\n",
        "    rays_o = c2w[:3, -1].expand(rays_d.shape)\n",
        "    \n",
        "    return rays_o, rays_d\n",
        "\n",
        "# Fix the render_rays function to handle float steps\n",
        "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, device, rand=False, embed_fn=None, chunk=64):\n",
        "    def batchify(fn, chunk):\n",
        "        return lambda inputs: torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
        "    \n",
        "    # Make sure all inputs are on the same device\n",
        "    device = rays_o.device  # Use the device of the input rays\n",
        "    \n",
        "    # Convert N_samples to int if it's a float\n",
        "    N_samples = int(N_samples)\n",
        "    \n",
        "    # Sampling with memory efficiency in mind\n",
        "    z_vals = torch.linspace(near, far, steps=N_samples, device=device)\n",
        "    noise = None  # Initialize noise as None\n",
        "    \n",
        "    if rand:\n",
        "        # Only add noise to each z_val when rand is True\n",
        "        noise = torch.rand(rays_o.shape[0], N_samples, device=device) * (far - near) / N_samples\n",
        "        z_vals = z_vals.unsqueeze(0).expand(rays_o.shape[0], N_samples) + noise\n",
        "    else:\n",
        "        z_vals = z_vals.unsqueeze(0).expand(rays_o.shape[0], N_samples)\n",
        "    \n",
        "    # More efficient way to create points\n",
        "    rays_o_shaped = rays_o.unsqueeze(1)  # [batch, 1, 3]\n",
        "    rays_d_shaped = rays_d.unsqueeze(1)  # [batch, 1, 3]\n",
        "    z_vals_shaped = z_vals.unsqueeze(-1)  # [batch, N_samples, 1]\n",
        "    \n",
        "    # More memory-efficient ray point generation\n",
        "    pts = rays_o_shaped + rays_d_shaped * z_vals_shaped  # [batch, N_samples, 3]\n",
        "    \n",
        "    # Normalize view directions and expand more efficiently\n",
        "    rays_d_norm = torch.nn.functional.normalize(rays_d, dim=-1)\n",
        "    view_dirs = rays_d_norm.unsqueeze(1).expand_as(pts)  # [batch, N_samples, 3]\n",
        "    \n",
        "    # Combine for network input\n",
        "    input_pts = torch.cat((pts, view_dirs), dim=-1)  # [batch, N_samples, 6]\n",
        "    \n",
        "    # Clear intermediate tensors to free memory\n",
        "    del pts, view_dirs, rays_o_shaped, rays_d_shaped, z_vals_shaped\n",
        "    \n",
        "    # Use smaller chunk size for network evaluation\n",
        "    raw = batchify(network_fn, chunk)(input_pts)\n",
        "    \n",
        "    # Release input_pts memory\n",
        "    del input_pts\n",
        "    \n",
        "    # The rest of the function remains similar\n",
        "    sigma_a = raw[...,0]  # Shape: [batch, N_samples]\n",
        "    rgb = raw[...,1:]  # Shape: [batch, N_samples, 3]\n",
        "    \n",
        "    # Improved volume rendering - compute dists more efficiently\n",
        "    dists = torch.cat([\n",
        "        z_vals[..., 1:] - z_vals[..., :-1],\n",
        "        torch.ones_like(z_vals[..., :1], device=device) * 1e10\n",
        "    ], dim=-1)\n",
        "    \n",
        "    alpha = 1. - torch.exp(-sigma_a * dists)\n",
        "    alpha = alpha.unsqueeze(-1)  # [batch, N_samples, 1]\n",
        "    \n",
        "    # Computing transmittance with a more memory-efficient approach\n",
        "    ones_shape = list(alpha.shape)\n",
        "    ones_shape[1] = 1\n",
        "    ones = torch.ones(ones_shape, device=device)\n",
        "    \n",
        "    # Compute transmittance efficiently\n",
        "    T = torch.cumprod(\n",
        "        torch.cat([ones, 1. - alpha + 1e-10], dim=1),\n",
        "        dim=1\n",
        "    )[:, :-1]  # [batch, N_samples, 1]\n",
        "    \n",
        "    weights = alpha * T\n",
        "    \n",
        "    # Compute final colors and depths\n",
        "    rgb_map = torch.sum(weights * rgb, dim=1)\n",
        "    depth_map = torch.sum(weights.squeeze(-1) * z_vals, dim=-1)\n",
        "    acc_map = torch.sum(weights.squeeze(-1), dim=-1)\n",
        "    \n",
        "    # Clear intermediate variables - only delete what exists\n",
        "    del raw, alpha, weights, T, sigma_a, rgb, dists, z_vals\n",
        "    if noise is not None:\n",
        "        del noise\n",
        "    \n",
        "    return rgb_map, depth_map, acc_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxUOrhIqyoUC"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSrFI-b-xt7d"
      },
      "outputs": [],
      "source": [
        "def train(images, poses, H, W, focal, testpose, testimg, device):\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = NeRF().to(device)\n",
        "    criterion = nn.MSELoss(reduction='mean')\n",
        "    # Start with a higher learning rate and decay it more gradually\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)  # Standard NeRF learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.995)  # Slower decay\n",
        "    n_iter = 30000\n",
        "    n_samples = 256  # Reduced from 64 to save memory\n",
        "    i_plot = 500\n",
        "    psnrs = []\n",
        "    iternums = []\n",
        "    t = time.time()\n",
        "\n",
        "    warmup_iterations = 1000\n",
        "    for i in range(n_iter):\n",
        "        # Learning rate warmup\n",
        "        if i < warmup_iterations:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = 5e-4 * min(1, i / warmup_iterations)\n",
        "    \n",
        "    # Reduce batch size and use smaller image dimensions for training\n",
        "    ray_batch_size = 1024\n",
        "    # During training: Moderate downscaling\n",
        "    downscale_factor = 1  # Use half resolution for training\n",
        "\n",
        "    # During testing: Use higher resolution for evaluation\n",
        "    test_downscale = 1  # Evaluate at higher resolution\n",
        "    \n",
        "    # Downscale dimensions for training\n",
        "    h_train = H // downscale_factor\n",
        "    w_train = W // downscale_factor\n",
        "    \n",
        "    # Convert data to tensors ONCE but keep on CPU\n",
        "    images_tensor = torch.from_numpy(images).float()\n",
        "    poses_tensor = torch.from_numpy(poses).float()\n",
        "    \n",
        "    # Make sure testimg and testpose are on the correct device\n",
        "    testimg = testimg.to(device)\n",
        "    testpose = testpose.to(device)\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "        # Select a random image for training\n",
        "        img_i = np.random.randint(images.shape[0])\n",
        "        \n",
        "        # Get target image and pose\n",
        "        target = images_tensor[img_i]  # Keep on CPU initially\n",
        "        pose = poses_tensor[img_i].to(device)  # Move to device only when needed\n",
        "        \n",
        "        # Downscale for training\n",
        "        if downscale_factor > 1:\n",
        "            target_resized = F.interpolate(target.permute(2, 0, 1).unsqueeze(0),\n",
        "                                         size=(h_train, w_train),\n",
        "                                         mode='bilinear').squeeze(0).permute(1, 2, 0)\n",
        "        else:\n",
        "            target_resized = target\n",
        "        \n",
        "        # Get rays for the downscaled image\n",
        "        rays_o, rays_d = get_rays(h_train, w_train, focal / downscale_factor, pose)\n",
        "        \n",
        "        # Use only a subset of rays for training (random sampling)\n",
        "        select_inds = np.random.choice(rays_o.shape[0], size=ray_batch_size, replace=False)\n",
        "        rays_o = rays_o[select_inds].to(device)\n",
        "        rays_d = rays_d[select_inds].to(device)\n",
        "        \n",
        "        # Move target to the same device as the model\n",
        "        target_s = target_resized.reshape(-1, 3)[select_inds].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Use smaller chunk size for processing\n",
        "        chunk_size = 256  # Smaller chunk size to reduce memory usage\n",
        "        \n",
        "        # All tensors should now be on the same device\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=7.,\n",
        "                                     N_samples=n_samples, device=device,\n",
        "                                     rand=True, chunk=chunk_size)\n",
        "        \n",
        "        # Double-check device consistency\n",
        "        if rgb.device != target_s.device:\n",
        "            print(f\"Warning: rgb on {rgb.device}, target_s on {target_s.device}\")\n",
        "            rgb = rgb.to(target_s.device)  # Move rgb to match target_s\n",
        "            \n",
        "        # Compute loss on the ray batch\n",
        "        loss = criterion(rgb, target_s)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Clean up to free memory\n",
        "        del rays_o, rays_d, rgb, depth, acc, target_s\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        \n",
        "        if i % i_plot == 0:\n",
        "            print(f'Iteration: {i}, Loss: {loss.item():.6f}, Time: {(time.time() - t) / i_plot:.2f} secs per iter')\n",
        "            t = time.time()\n",
        "            \n",
        "            # Evaluate on a subset of the test image to save memory\n",
        "            test_downscale = 8  # Higher downscale for visualization during training\n",
        "            test_h, test_w = H // test_downscale, W // test_downscale\n",
        "            test_ray_batch_size = 16384  # Process more rays at once for testing\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                # Get downscaled rays for test image\n",
        "                test_rays_o, test_rays_d = get_rays(test_h, test_w, focal / test_downscale, testpose)\n",
        "                \n",
        "                # Process test rays in smaller batches\n",
        "                test_rgb_parts = []\n",
        "                for j in range(0, test_rays_o.shape[0], test_ray_batch_size):\n",
        "                    end_idx = min(j + test_ray_batch_size, test_rays_o.shape[0])\n",
        "                    batch_o = test_rays_o[j:end_idx].to(device)\n",
        "                    batch_d = test_rays_d[j:end_idx].to(device)\n",
        "                    \n",
        "                    # Render with the model\n",
        "                    rgb_batch, _, _ = render_rays(\n",
        "                        model, batch_o, batch_d, near=2., far=6.,\n",
        "                        N_samples=32, device=device, chunk=chunk_size\n",
        "                    )\n",
        "                    \n",
        "                    # Move results to CPU immediately\n",
        "                    test_rgb_parts.append(rgb_batch.cpu())\n",
        "                    \n",
        "                    # Clean up batch\n",
        "                    del batch_o, batch_d, rgb_batch\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "                \n",
        "                # Combine batches on CPU\n",
        "                test_rgb = torch.cat(test_rgb_parts, dim=0)\n",
        "                test_rgb = test_rgb.reshape(test_h, test_w, 3)\n",
        "                \n",
        "                # Resize test image to match current resolution for comparison\n",
        "                testimg_resized = F.interpolate(\n",
        "                    testimg.permute(2, 0, 1).unsqueeze(0),\n",
        "                    size=(test_h, test_w),\n",
        "                    mode='bilinear'\n",
        "                ).squeeze(0).permute(1, 2, 0).cpu()\n",
        "                \n",
        "                # Compute PSNR on CPU\n",
        "                test_loss = F.mse_loss(test_rgb, testimg_resized)\n",
        "                psnr = -10. * torch.log10(test_loss)\n",
        "                \n",
        "                psnrs.append(psnr.item())\n",
        "                iternums.append(i)\n",
        "                \n",
        "                # Plot\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.subplot(121)\n",
        "                plt.imshow(test_rgb.numpy())\n",
        "                plt.title(f'Iteration: {i}')\n",
        "                plt.subplot(122)\n",
        "                plt.plot(iternums, psnrs)\n",
        "                plt.title('PSNR')\n",
        "                plt.show()\n",
        "                \n",
        "                # Clean up\n",
        "                del test_rgb, test_rgb_parts, test_loss, testimg_resized\n",
        "                gc.collect()\n",
        "        \n",
        "        # Step learning rate scheduler\n",
        "        if (i + 1) % 100 == 0:\n",
        "            scheduler.step()\n",
        "            \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeuFBERhzQsl"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aXUYCNIAZTR"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = np.load('nerf_dataset/nerf_dataset.npz')\n",
        "images = data['images'].astype(np.float32)\n",
        "poses = data['poses'].astype(np.float32)\n",
        "focal = data['focal'].astype(np.float32)  # Convert focal to float32\n",
        "H, W = images.shape[1:3]\n",
        "print(images.shape, poses.shape, focal)\n",
        "\n",
        "# Check if MPS is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "    \n",
        "# Split into training and test sets\n",
        "# Use the last image as test image\n",
        "test_idx = -1  # Use the last image as the test image\n",
        "testimg = images[test_idx]\n",
        "testpose = poses[test_idx]\n",
        "\n",
        "# Use all other images for training (excluding the test image)\n",
        "if test_idx == -1:\n",
        "    train_images = images[:-1]\n",
        "    train_poses = poses[:-1]\n",
        "else:\n",
        "    # If using a specific index\n",
        "    train_images = np.concatenate([images[:test_idx], images[test_idx+1:]], axis=0)\n",
        "    train_poses = np.concatenate([poses[:test_idx], poses[test_idx+1:]], axis=0)\n",
        "\n",
        "# Ensure images have 3 channels (RGB)\n",
        "if train_images.shape[-1] > 3:\n",
        "    print(f\"Original image shape: {train_images.shape}, trimming to 3 channels\")\n",
        "    train_images = train_images[...,:3]\n",
        "    testimg = testimg[...,:3]\n",
        "\n",
        "# Display test image\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(testimg)\n",
        "plt.title(\"Test Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Convert tensors to the selected device with float32 dtype\n",
        "testimg = torch.from_numpy(testimg).float().to(device)\n",
        "testpose = torch.from_numpy(testpose).float().to(device)\n",
        "\n",
        "# Now use train_images and train_poses in your training loop\n",
        "print(f\"Training set: {train_images.shape} images\")\n",
        "print(f\"Test image: {testimg.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv9MgPOuEBdv"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpZk-t87yIYU"
      },
      "outputs": [],
      "source": [
        "model = train(train_images, train_poses, H, W, focal, testpose, testimg, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6LcY64I4PUW"
      },
      "source": [
        "## Render Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmuvZJbu4btG"
      },
      "outputs": [],
      "source": [
        "# Transformation matrices in PyTorch\n",
        "trans_t = lambda t: torch.tensor([\n",
        "    [1, 0, 0, 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 1, t],\n",
        "    [0, 0, 0, 1]\n",
        "], dtype=torch.float32, device=device)\n",
        "\n",
        "rot_phi = lambda phi: torch.tensor([\n",
        "    [1, 0, 0, 0],\n",
        "    [0, torch.cos(phi), -torch.sin(phi), 0],\n",
        "    [0, torch.sin(phi), torch.cos(phi), 0],\n",
        "    [0, 0, 0, 1]\n",
        "], dtype=torch.float32, device=device)\n",
        "\n",
        "rot_theta = lambda th: torch.tensor([\n",
        "    [torch.cos(th), 0, -torch.sin(th), 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [torch.sin(th), 0, torch.cos(th), 0],\n",
        "    [0, 0, 0, 1]\n",
        "], dtype=torch.float32, device=device)\n",
        "\n",
        "# Pose function with spherical coordinates\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = torch.matmul(rot_phi(torch.tensor([phi / 180. * np.pi], dtype=torch.float32, device=device)), c2w)\n",
        "    c2w = torch.matmul(rot_theta(torch.tensor([theta / 180. * np.pi], dtype=torch.float32, device=device)), c2w)\n",
        "    c2w = torch.tensor([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]],\n",
        "                      dtype=torch.float32, device=device) @ c2w\n",
        "    return c2w\n",
        "\n",
        "# Enhanced rendering function with debug information\n",
        "def f_optimized(**kwargs):\n",
        "    # Extract parameters and convert float sliders to appropriate types\n",
        "    near = float(kwargs.pop('near', 2.))\n",
        "    far = float(kwargs.pop('far', 6.))\n",
        "    n_samples = int(kwargs.pop('n_samples', 32))\n",
        "    downscale = int(kwargs.pop('downscale', 4))\n",
        "    \n",
        "    # Get camera pose\n",
        "    c2w = pose_spherical(**kwargs)\n",
        "    \n",
        "    # Print debug info\n",
        "    print(f\"Camera position: {c2w[0,3].item():.2f}, {c2w[1,3].item():.2f}, {c2w[2,3].item():.2f}\")\n",
        "    print(f\"Rendering with: near={near}, far={far}, samples={n_samples}, downscale={downscale}\")\n",
        "    \n",
        "    # Use a lower resolution for interactive visualization\n",
        "    h, w = H // downscale, W // downscale\n",
        "    \n",
        "    # Get rays with downsampling\n",
        "    rays_o, rays_d = get_rays(h, w, focal / downscale, c2w[:3, :4])\n",
        "    c2w, rays_o, rays_d = map(lambda t: t.to(device), (c2w, rays_o, rays_d))\n",
        "    \n",
        "    # Process rays in batches to save memory\n",
        "    ray_batch_size = 8192\n",
        "    total_rays = rays_o.shape[0]\n",
        "    rgb_parts = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(0, total_rays, ray_batch_size):\n",
        "            # Get ray batch\n",
        "            end_idx = min(i + ray_batch_size, total_rays)\n",
        "            batch_rays_o = rays_o[i:end_idx]\n",
        "            batch_rays_d = rays_d[i:end_idx]\n",
        "            \n",
        "            # Render with specified parameters\n",
        "            rgb_batch, _, _ = render_rays(\n",
        "                model, \n",
        "                batch_rays_o, \n",
        "                batch_rays_d, \n",
        "                near=near,\n",
        "                far=far,\n",
        "                N_samples=n_samples,\n",
        "                device=device, \n",
        "                chunk=64\n",
        "            )\n",
        "            \n",
        "            # Move results to CPU immediately\n",
        "            rgb_parts.append(rgb_batch.cpu())\n",
        "            \n",
        "            # Clean up batch\n",
        "            del batch_rays_o, batch_rays_d, rgb_batch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            import gc\n",
        "            gc.collect()\n",
        "        \n",
        "        # Combine batches on CPU\n",
        "        rgb = torch.cat(rgb_parts, dim=0)\n",
        "        rgb = rgb.reshape(h, w, 3)\n",
        "        \n",
        "        # Clamp and convert to numpy\n",
        "        img = torch.clamp(rgb, 0, 1).numpy()\n",
        "        \n",
        "        # Display at a reasonable size\n",
        "        plt.figure(figsize=(12, 12 * h / w))\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"θ={kwargs['theta']:.1f}°, φ={kwargs['phi']:.1f}°, r={kwargs['radius']:.1f}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "        # Clean up\n",
        "        del rgb, rgb_parts\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc  # Add this at the top with your other imports\n",
        "\n",
        "# Try rendering with the test pose that was used during training\n",
        "def render_with_testpose():\n",
        "    with torch.no_grad():\n",
        "        downscale = 4  # Lower resolution for quick viewing\n",
        "        h, w = H // downscale, W // downscale\n",
        "        \n",
        "        # Get rays from the testpose\n",
        "        rays_o, rays_d = get_rays(h, w, focal / downscale, testpose[:3, :4])\n",
        "        rays_o, rays_d = rays_o.to(device), rays_d.to(device)\n",
        "        \n",
        "        # Process in batches\n",
        "        ray_batch_size = 8192\n",
        "        total_rays = rays_o.shape[0]\n",
        "        rgb_parts = []\n",
        "        \n",
        "        for i in range(0, total_rays, ray_batch_size):\n",
        "            end_idx = min(i + ray_batch_size, total_rays)\n",
        "            batch_o = rays_o[i:end_idx]\n",
        "            batch_d = rays_d[i:end_idx]\n",
        "            \n",
        "            rgb_batch, _, _ = render_rays(\n",
        "                model, batch_o, batch_d, near=2., far=6.,\n",
        "                N_samples=32, device=device, chunk=64\n",
        "            )\n",
        "            \n",
        "            rgb_parts.append(rgb_batch.cpu())\n",
        "            del batch_o, batch_d, rgb_batch\n",
        "            gc.collect()\n",
        "        \n",
        "        # Combine and reshape\n",
        "        rgb = torch.cat(rgb_parts, dim=0).reshape(h, w, 3)\n",
        "        \n",
        "        # Display\n",
        "        img = torch.clamp(rgb, 0, 1).numpy()\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Using testpose from training\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "        # Clean up\n",
        "        del rgb, rgb_parts\n",
        "        gc.collect()\n",
        "\n",
        "# Call this function to see the result\n",
        "render_with_testpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interactive, widgets\n",
        "\n",
        "# Simplified interactive viewer with hardcoded defaults\n",
        "def render_view(theta, phi, radius, near, far, n_samples, downscale):\n",
        "    with torch.no_grad():\n",
        "        # Get camera pose\n",
        "        c2w = pose_spherical(theta, phi, radius)\n",
        "        \n",
        "        # Use a lower resolution for interactive visualization\n",
        "        h, w = H // downscale, W // downscale\n",
        "        \n",
        "        # Get rays with downsampling\n",
        "        rays_o, rays_d = get_rays(h, w, focal / downscale, c2w[:3, :4])\n",
        "        rays_o, rays_d = rays_o.to(device), rays_d.to(device)\n",
        "        \n",
        "        # Process rays in batches to save memory\n",
        "        ray_batch_size = 8192\n",
        "        total_rays = rays_o.shape[0]\n",
        "        rgb_parts = []\n",
        "        \n",
        "        for i in range(0, total_rays, ray_batch_size):\n",
        "            # Get ray batch\n",
        "            end_idx = min(i + ray_batch_size, total_rays)\n",
        "            batch_o = rays_o[i:end_idx]\n",
        "            batch_d = rays_d[i:end_idx]\n",
        "            \n",
        "            # Render with specified parameters\n",
        "            rgb_batch, _, _ = render_rays(\n",
        "                model, \n",
        "                batch_o, \n",
        "                batch_d, \n",
        "                near=near,\n",
        "                far=far,\n",
        "                N_samples=int(n_samples),\n",
        "                device=device, \n",
        "                chunk=64\n",
        "            )\n",
        "            \n",
        "            # Move results to CPU immediately\n",
        "            rgb_parts.append(rgb_batch.cpu())\n",
        "            \n",
        "            # Clean up batch\n",
        "            del batch_o, batch_d, rgb_batch\n",
        "            gc.collect()\n",
        "        \n",
        "        # Combine batches on CPU\n",
        "        rgb = torch.cat(rgb_parts, dim=0)\n",
        "        rgb = rgb.reshape(h, w, 3)\n",
        "        \n",
        "        # Clamp and convert to numpy\n",
        "        img = torch.clamp(rgb, 0, 1).numpy()\n",
        "        \n",
        "        # Display at a reasonable size\n",
        "        plt.figure(figsize=(12, 12 * h / w))\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"θ={theta:.1f}°, φ={phi:.1f}°, r={radius:.1f}, near={near}, far={far}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "        # Clean up\n",
        "        del rgb, rgb_parts\n",
        "        gc.collect()\n",
        "\n",
        "# Create a simpler interactive viewer with hardcoded values\n",
        "interactive_widget = interactive(\n",
        "    render_view,\n",
        "    theta=widgets.FloatSlider(value=100.0, min=0.0, max=360.0, step=5.0, description='Theta'),\n",
        "    phi=widgets.FloatSlider(value=-30.0, min=-90.0, max=0.0, step=5.0, description='Phi'),\n",
        "    radius=widgets.FloatSlider(value=6.0, min=2.0, max=12.0, step=0.5, description='Radius'),\n",
        "    near=widgets.FloatSlider(value=1.0, min=0.1, max=5.0, step=0.1, description='Near'),\n",
        "    far=widgets.FloatSlider(value=10.0, min=3.0, max=20.0, step=1.0, description='Far'),\n",
        "    n_samples=widgets.IntSlider(value=32, min=16, max=64, step=8, description='Samples'),\n",
        "    downscale=widgets.IntSlider(value=4, min=2, max=8, step=1, description='Downscale')\n",
        ")\n",
        "\n",
        "# Display the widget\n",
        "display(interactive_widget)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eE2h-hl4Vli"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "for th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n",
        "    c2w = pose_spherical(th, -30., 4.)\n",
        "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
        "    c2w, rays_o, rays_d = map(lambda t: t.to(device), (c2w, rays_o, rays_d))\n",
        "    with torch.no_grad():\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=32, device=device)\n",
        "        rgb = rgb.reshape(H, W, 3)\n",
        "        frames.append((255*np.clip(rgb.cpu().detach().numpy(),0,1)).astype(np.uint8))\n",
        "\n",
        "import imageio\n",
        "f = 'video.mp4'\n",
        "imageio.mimwrite(f, frames, fps=30, quality=7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import imageio\n",
        "f = 'video.gif'\n",
        "imageio.mimwrite(f, frames, fps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZNGouEH4TOJ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9PaCftwbyPpZ",
        "yyZgyFadyW5u"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NeRF",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
