{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# PyTorch3D imports\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    SoftPhongShader,\n",
    "    TexturesVertex\n",
    ")\n",
    "from pytorch3d.io import load_obj, save_obj\n",
    "\n",
    "# Set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: CPU only, this will be slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and preprocess images\n",
    "def load_images_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Load all images from the specified folder and resize them to a consistent size\n",
    "    \"\"\"\n",
    "    target_size = (224, 224)  # We can adjust this size based on our needs\n",
    "    images = []\n",
    "    \n",
    "    # Get all image files from the folder\n",
    "    image_paths = glob.glob(os.path.join(folder_path, \"*.[jJ][pP][gG]\")) + \\\n",
    "                 glob.glob(os.path.join(folder_path, \"*.[pP][nN][gG]\"))\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        try:\n",
    "            # Open and convert to RGB (in case of RGBA or grayscale)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Resize image while maintaining aspect ratio\n",
    "            img.thumbnail(target_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Convert to tensor and normalize to [0, 1]\n",
    "            img_tensor = torch.FloatTensor(np.array(img)) / 255.0\n",
    "            images.append(img_tensor)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {str(e)}\")\n",
    "    \n",
    "    return images\n",
    "\n",
    "# Load images from your media folder\n",
    "media_path = \"./media\"  # Update this path to your media folder location\n",
    "input_images = load_images_from_folder(media_path)\n",
    "\n",
    "print(f\"Loaded {len(input_images)} images\")\n",
    "\n",
    "# Display some of the loaded images\n",
    "fig, axes = plt.subplots(1, min(5, len(input_images)), figsize=(15, 3))\n",
    "for i, img_tensor in enumerate(input_images[:5]):\n",
    "    if len(input_images) > 1:\n",
    "        ax = axes[i]\n",
    "    else:\n",
    "        ax = axes\n",
    "    ax.imshow(img_tensor)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2222e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Enhanced image loading and preprocessing\n",
    "def analyze_image_dataset(images):\n",
    "    \"\"\"Analyze the loaded images and print useful statistics\"\"\"\n",
    "    n_images = len(images)\n",
    "    image_sizes = [img.shape for img in images]\n",
    "    unique_sizes = set(str(size) for size in image_sizes)\n",
    "    \n",
    "    print(f\"Dataset statistics:\")\n",
    "    print(f\"Number of images: {n_images}\")\n",
    "    print(f\"Unique image sizes: {len(unique_sizes)}\")\n",
    "    for size in unique_sizes:\n",
    "        count = sum(1 for img_size in image_sizes if str(img_size) == size)\n",
    "        print(f\"  {size}: {count} images\")\n",
    "    \n",
    "    # Calculate mean and std for normalization\n",
    "    all_images = torch.stack([img for img in images])\n",
    "    mean = all_images.mean(dim=[0, 1, 2])\n",
    "    std = all_images.std(dim=[0, 1, 2])\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# Analyze the loaded dataset\n",
    "mean, std = analyze_image_dataset(input_images)\n",
    "\n",
    "# Create a subset for initial testing (optional)\n",
    "n_test_images = 20  # Adjust based on your computational resources\n",
    "test_indices = torch.linspace(0, len(input_images)-1, n_test_images).long()\n",
    "test_images = [input_images[i] for i in test_indices]\n",
    "\n",
    "# Display mean values per channel\n",
    "print(\"\\nChannel-wise mean values:\")\n",
    "print(f\"R: {mean[0]:.3f}, G: {mean[1]:.3f}, B: {mean[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 (Updated): Initialize base mesh and renderer configuration\n",
    "def create_base_mesh(device):\n",
    "    \"\"\"\n",
    "    Create a simple spherical mesh as our starting point for deformation.\n",
    "    \"\"\"\n",
    "    from pytorch3d.utils import ico_sphere\n",
    "    \n",
    "    # Create sphere of radius 1\n",
    "    base_mesh = ico_sphere(4, device) # Level 4 subdivision for reasonable detail\n",
    "    \n",
    "    # Initialize vertex colors/texture for visualization\n",
    "    verts = base_mesh.verts_padded()\n",
    "    N = verts.shape[1]\n",
    "    verts_rgb = torch.ones((1, N, 3), device=device)  # White color base\n",
    "    textures = TexturesVertex(verts_features=verts_rgb)\n",
    "    \n",
    "    base_mesh.textures = textures\n",
    "    return base_mesh\n",
    "\n",
    "# Initialize our renderer\n",
    "def create_renderer(image_size=(126, 224), device=device):\n",
    "    \"\"\"\n",
    "    Create a renderer with our desired settings\n",
    "    \"\"\"\n",
    "    # Camera settings\n",
    "    cameras = FoVPerspectiveCameras(device=device)\n",
    "    \n",
    "    # Rasterization settings\n",
    "    raster_settings = RasterizationSettings(\n",
    "        image_size=image_size,  # Match input image dimensions\n",
    "        blur_radius=0.0,\n",
    "        faces_per_pixel=1,\n",
    "    )\n",
    "    \n",
    "    # Lighting\n",
    "    lights = PointLights(\n",
    "        device=device,\n",
    "        location=[[0.0, 0.0, -3.0]],\n",
    "        ambient_color=((0.5, 0.5, 0.5),),\n",
    "        diffuse_color=((0.3, 0.3, 0.3),),\n",
    "        specular_color=((0.2, 0.2, 0.2),),\n",
    "    )\n",
    "    \n",
    "    renderer = MeshRenderer(\n",
    "        rasterizer=MeshRasterizer(\n",
    "            cameras=cameras, \n",
    "            raster_settings=raster_settings\n",
    "        ),\n",
    "        shader=SoftPhongShader(\n",
    "            device=device, \n",
    "            cameras=cameras,\n",
    "            lights=lights\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return renderer\n",
    "\n",
    "# Create base mesh and renderer\n",
    "base_mesh = create_base_mesh(device)\n",
    "renderer = create_renderer(image_size=(126, 224), device=device)\n",
    "\n",
    "# Visualize initial mesh from a few viewpoints\n",
    "def visualize_mesh(mesh, renderer, num_views=3):\n",
    "    elevs = torch.linspace(0, 360, num_views)\n",
    "    azims = torch.linspace(-180, 180, num_views)\n",
    "    \n",
    "    images = []\n",
    "    for elev, azim in zip(elevs, azims):\n",
    "        R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "        cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "        \n",
    "        image = renderer(mesh, cameras=cameras)\n",
    "        images.append(image[0, ..., :3].cpu().detach())\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, num_views, figsize=(15, 5))\n",
    "    for i, img in enumerate(images):\n",
    "        if num_views > 1:\n",
    "            ax = axes[i]\n",
    "        else:\n",
    "            ax = axes\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize our starting point\n",
    "visualize_mesh(base_mesh, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 (Updated): Define the mesh deformation model and losses\n",
    "class MeshDeformationModel(torch.nn.Module):\n",
    "    def __init__(self, base_mesh, device=device):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Get initial vertices and create learnable offsets\n",
    "        self.initial_verts = base_mesh.verts_packed().clone()\n",
    "        self.deform_verts = torch.nn.Parameter(\n",
    "            torch.zeros_like(self.initial_verts, device=device)\n",
    "        )\n",
    "        \n",
    "        # Keep faces fixed - we only deform vertices\n",
    "        self.register_buffer('faces', base_mesh.faces_packed())\n",
    "        \n",
    "        # Initialize vertex colors as learnable parameters\n",
    "        N = len(self.initial_verts)\n",
    "        self.vertex_colors = torch.nn.Parameter(\n",
    "            torch.ones((N, 3), device=device) * 0.5\n",
    "        )\n",
    "\n",
    "    def forward(self):\n",
    "        # Apply deformation to vertices\n",
    "        deformed_verts = self.initial_verts + self.deform_verts\n",
    "        \n",
    "        # Create texture from vertex colors\n",
    "        textures = TexturesVertex(\n",
    "            verts_features=self.vertex_colors[None]\n",
    "        )\n",
    "        \n",
    "        # Create and return mesh\n",
    "        return Meshes(\n",
    "            verts=[deformed_verts],\n",
    "            faces=[self.faces],\n",
    "            textures=textures\n",
    "        )\n",
    "\n",
    "class ReconstructionLoss:\n",
    "    def __init__(self, renderer):\n",
    "        self.renderer = renderer\n",
    "    \n",
    "    def rgb_loss(self, pred_rgb, target_rgb):\n",
    "        \"\"\"Simple L1 loss between RGB images\"\"\"\n",
    "        # Ensure both tensors are on the same device\n",
    "        target_rgb = target_rgb.to(pred_rgb.device)\n",
    "        return torch.abs(pred_rgb - target_rgb).mean()\n",
    "    \n",
    "    def silhouette_loss(self, pred_silhouette, target_silhouette):\n",
    "        \"\"\"Binary cross entropy loss for silhouettes\"\"\"\n",
    "        # Ensure both tensors are on the same device\n",
    "        target_silhouette = target_silhouette.to(pred_silhouette.device)\n",
    "        return torch.nn.functional.binary_cross_entropy(\n",
    "            pred_silhouette.clamp(min=0.0, max=1.0),\n",
    "            target_silhouette\n",
    "        )\n",
    "    \n",
    "    def mesh_regularization(self, mesh):\n",
    "        \"\"\"Regularization to encourage smooth mesh\"\"\"\n",
    "        from pytorch3d.loss import mesh_laplacian_smoothing\n",
    "        return mesh_laplacian_smoothing(mesh)\n",
    "    \n",
    "    def edge_regularization(self, mesh):\n",
    "        \"\"\"Regularization to prevent long edges\"\"\"\n",
    "        from pytorch3d.loss import mesh_edge_loss\n",
    "        return mesh_edge_loss(mesh)\n",
    "\n",
    "    def compute_loss(self, mesh, target_image, camera):\n",
    "        \"\"\"Compute full loss for a single view\"\"\"\n",
    "        # Render predicted image\n",
    "        pred_image = self.renderer(mesh, cameras=camera)\n",
    "        \n",
    "        # Extract RGB and silhouette\n",
    "        pred_rgb = pred_image[..., :3]\n",
    "        pred_silhouette = pred_image[..., 3]\n",
    "        \n",
    "        # Convert target image to silhouette (simple threshold)\n",
    "        target_silhouette = (target_image.mean(dim=-1) > 0.1).float()\n",
    "        \n",
    "        # Compute losses\n",
    "        rgb_loss = self.rgb_loss(pred_rgb[0], target_image)\n",
    "        sil_loss = self.silhouette_loss(pred_silhouette[0], target_silhouette)\n",
    "        lap_loss = self.mesh_regularization(mesh)\n",
    "        edge_loss = self.edge_regularization(mesh)\n",
    "        \n",
    "        # Combine losses with weights\n",
    "        total_loss = (\n",
    "            1.0 * rgb_loss + \n",
    "            1.0 * sil_loss + \n",
    "            0.1 * lap_loss + \n",
    "            0.1 * edge_loss\n",
    "        )\n",
    "        \n",
    "        return total_loss, {\n",
    "            'rgb_loss': rgb_loss.item(),\n",
    "            'silhouette_loss': sil_loss.item(),\n",
    "            'laplacian_loss': lap_loss.item(),\n",
    "            'edge_loss': edge_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "# Initialize model and loss\n",
    "model = MeshDeformationModel(base_mesh, device=device)\n",
    "loss_fn = ReconstructionLoss(renderer)\n",
    "\n",
    "# Test forward pass and loss computation\n",
    "test_mesh = model()\n",
    "R, T = look_at_view_transform(dist=2.7, elev=0, azim=0)\n",
    "test_camera = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "test_loss, loss_dict = loss_fn.compute_loss(test_mesh, input_images[0], test_camera)\n",
    "\n",
    "print(\"Initial test loss values:\")\n",
    "for k, v in loss_dict.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Visualize initial state\n",
    "visualize_mesh(test_mesh, renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a02de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training loop with CPU optimizations\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import tqdm.notebook\n",
    "\n",
    "class TrainingManager:\n",
    "    def __init__(self, model, loss_fn, optimizer, input_images, \n",
    "                 save_dir='./checkpoints', device=device):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.input_images = input_images\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "        self.device = device\n",
    "        \n",
    "        # Training history\n",
    "        self.history = defaultdict(list)\n",
    "        self.best_loss = float('inf')\n",
    "        \n",
    "    def train_step(self, target_images, n_views=2):\n",
    "        \"\"\"Single training step with memory-efficient view sampling\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Sample a subset of views for this step\n",
    "        elevs = torch.linspace(0, 360, n_views)\n",
    "        azims = torch.linspace(-180, 180, n_views)\n",
    "        \n",
    "        # Get current mesh\n",
    "        mesh = self.model()\n",
    "        \n",
    "        # Initialize total loss\n",
    "        total_loss = 0\n",
    "        all_losses = defaultdict(float)\n",
    "        \n",
    "        # Process each view sequentially to save memory\n",
    "        for view_idx, (elev, azim) in enumerate(zip(elevs, azims)):\n",
    "            # Create camera for this view\n",
    "            R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "            camera = FoVPerspectiveCameras(device=self.device, R=R, T=T)\n",
    "            \n",
    "            # Get target image\n",
    "            target_idx = view_idx % len(target_images)\n",
    "            target = target_images[target_idx]\n",
    "            \n",
    "            # Compute loss for this view\n",
    "            loss, losses = self.loss_fn.compute_loss(mesh, target, camera)\n",
    "            total_loss += loss / n_views\n",
    "            \n",
    "            # Accumulate losses\n",
    "            for k, v in losses.items():\n",
    "                all_losses[k] += v / n_views\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return dict(all_losses)\n",
    "    \n",
    "    def train(self, n_iterations=1000, batch_size=4, \n",
    "              log_every=10, save_every=100):\n",
    "        \"\"\"Main training loop with progress tracking\"\"\"\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm.notebook.trange(n_iterations)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            for iteration in pbar:\n",
    "                # Select random subset of images for this iteration\n",
    "                batch_indices = torch.randperm(len(self.input_images))[:batch_size]\n",
    "                batch_images = [self.input_images[i] for i in batch_indices]\n",
    "                \n",
    "                # Training step\n",
    "                losses = self.train_step(batch_images)\n",
    "                \n",
    "                # Update history\n",
    "                for k, v in losses.items():\n",
    "                    self.history[k].append(v)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f\"{losses['total_loss']:.4f}\",\n",
    "                    'rgb_loss': f\"{losses['rgb_loss']:.4f}\"\n",
    "                })\n",
    "                \n",
    "                # Periodic logging\n",
    "                if iteration % log_every == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    print(f\"\\nIteration {iteration}\")\n",
    "                    print(f\"Time elapsed: {elapsed:.1f}s\")\n",
    "                    for k, v in losses.items():\n",
    "                        print(f\"{k}: {v:.4f}\")\n",
    "                    \n",
    "                    # Visualize current state\n",
    "                    with torch.no_grad():\n",
    "                        current_mesh = self.model()\n",
    "                        visualize_mesh(current_mesh, renderer)\n",
    "                \n",
    "                # Save checkpoint if best so far or periodic\n",
    "                if (losses['total_loss'] < self.best_loss or \n",
    "                    iteration % save_every == 0):\n",
    "                    self.save_checkpoint(iteration, losses)\n",
    "                    if losses['total_loss'] < self.best_loss:\n",
    "                        self.best_loss = losses['total_loss']\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def save_checkpoint(self, iteration, losses):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint = {\n",
    "            'iteration': iteration,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'losses': dict(self.history),\n",
    "            'best_loss': self.best_loss\n",
    "        }\n",
    "        path = self.save_dir / f'checkpoint_{iteration:04d}.pt'\n",
    "        torch.save(checkpoint, path)\n",
    "        print(f\"\\nSaved checkpoint to {path}\")\n",
    "\n",
    "# Initialize training\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.deform_verts, 'lr': 1e-3},\n",
    "    {'params': model.vertex_colors, 'lr': 1e-3}\n",
    "])\n",
    "\n",
    "trainer = TrainingManager(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    input_images=input_images,\n",
    "    save_dir='./reconstruction_checkpoints'\n",
    ")\n",
    "\n",
    "# Start training\n",
    "history = trainer.train(\n",
    "    n_iterations=1000,  # Reduced for CPU\n",
    "    batch_size=4,      # Small batch size for CPU\n",
    "    log_every=20,\n",
    "    save_every=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c5f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177d57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newenv] *",
   "language": "python",
   "name": "conda-env-newenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
