{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d852d7da-7fe1-46bc-bf95-27ede4001bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_dtype(torch.float32)\n",
    "# Ensure numpy arrays are also float32 by default\n",
    "np.set_printoptions(precision=6)\n",
    "\n",
    "# Define this as a global variable to use across functions\n",
    "mask_generator = None\n",
    "\n",
    "# Load the SAM model - using MPS for macOS GPU by default\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  # Using MPS for macOS GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_sam_model(device=\"mps\"):\n",
    "    \"\"\"Load the SAM model on the specified device.\"\"\"\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    sam.to(device=device)\n",
    "    \n",
    "    # Create the mask generator with same parameters as in the notebook\n",
    "    mask_gen = SamAutomaticMaskGenerator(\n",
    "        model=sam,\n",
    "        points_per_side=32,\n",
    "        pred_iou_thresh=0.9,\n",
    "        stability_score_thresh=0.96,\n",
    "        crop_n_layers=1,\n",
    "        crop_n_points_downscale_factor=2,\n",
    "        min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
    "    )\n",
    "    return mask_gen\n",
    "\n",
    "# Initialize global mask generator\n",
    "mask_generator = load_sam_model(device)\n",
    "\n",
    "def apply_masks_to_frame(frame, masks):\n",
    "    \"\"\"Apply colored masks to a frame.\"\"\"\n",
    "    # Create a transparent overlay\n",
    "    mask_overlay = np.ones((frame.shape[0], frame.shape[1], 4), dtype=np.float32)\n",
    "    mask_overlay[:, :, 3] = 0  # Set alpha channel to 0 (transparent)\n",
    "    \n",
    "    # Sort masks by area for better visualization (larger masks behind smaller ones)\n",
    "    sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "    \n",
    "    # Apply each mask with a random color\n",
    "    for mask in sorted_masks:\n",
    "        m = mask['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])  # RGB + alpha\n",
    "        mask_overlay[m] = color_mask\n",
    "    \n",
    "    # Convert frame to RGBA\n",
    "    frame_rgba = np.ones((frame.shape[0], frame.shape[1], 4), dtype=np.float32)\n",
    "    frame_rgba[:, :, :3] = frame / 255.0\n",
    "    frame_rgba[:, :, 3] = 1.0  # Fully opaque\n",
    "    \n",
    "    # Combine the two images (background frame and mask overlay)\n",
    "    composite = frame_rgba * (1 - mask_overlay[:, :, 3:4]) + mask_overlay * mask_overlay[:, :, 3:4]\n",
    "    \n",
    "    # Convert back to uint8 RGB\n",
    "    result = (composite[:, :, :3] * 255).astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "def create_binary_mask_visualization(frame_shape, masks):\n",
    "    \"\"\"Create a visualization where each mask gets a different color.\"\"\"\n",
    "    # Create an empty image\n",
    "    vis = np.zeros((*frame_shape[:2], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Sort masks by area\n",
    "    sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "    \n",
    "    # Assign a random color to each mask\n",
    "    for i, mask in enumerate(sorted_masks):\n",
    "        color = np.random.randint(0, 255, 3, dtype=np.uint8)\n",
    "        vis[mask['segmentation']] = color\n",
    "    \n",
    "    return vis\n",
    "\n",
    "def process_video(input_video_path, output_segmentation_path, output_masks_path, use_cpu=False):\n",
    "    global mask_generator\n",
    "    \n",
    "    # If forcing CPU, reload the model on CPU\n",
    "    if use_cpu and device != \"cpu\":\n",
    "        print(\"Switching to CPU as requested\")\n",
    "        mask_generator = load_sam_model(\"cpu\")\n",
    "    \n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height}, {fps} fps, {total_frames} frames\")\n",
    "    \n",
    "    # Create output video writers\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    segmentation_writer = cv2.VideoWriter(output_segmentation_path, fourcc, fps, (width, height))\n",
    "    masks_writer = cv2.VideoWriter(output_masks_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process each frame\n",
    "    frame_count = 0\n",
    "    progress_bar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert from BGR to RGB (SAM expects RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Ensure the frame is in float32 format\n",
    "        frame_rgb = frame_rgb.astype(np.float32) / 255.0\n",
    "        frame_rgb = (frame_rgb * 255.0).astype(np.uint8)\n",
    "        \n",
    "        # Generate masks for the current frame\n",
    "        try:\n",
    "            masks = mask_generator.generate(frame_rgb)\n",
    "            print(f\"Frame {frame_count}: Generated {len(masks)} masks\")\n",
    "        except TypeError as e:\n",
    "            if \"Cannot convert a MPS Tensor to float64\" in str(e):\n",
    "                # Fall back to CPU if MPS has float64 issues\n",
    "                print(\"MPS float64 issue detected. Falling back to CPU for this frame.\")\n",
    "                mask_generator = load_sam_model(\"cpu\")\n",
    "                masks = mask_generator.generate(frame_rgb)\n",
    "                print(f\"Frame {frame_count}: Generated {len(masks)} masks (CPU fallback)\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Create visualization with segmentation overlay\n",
    "        segmentation_frame = apply_masks_to_frame(frame_rgb, masks)\n",
    "        segmentation_frame_bgr = cv2.cvtColor(segmentation_frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Create mask visualization\n",
    "        masks_vis = create_binary_mask_visualization(frame.shape, masks)\n",
    "        \n",
    "        # Write to output videos\n",
    "        segmentation_writer.write(segmentation_frame_bgr)\n",
    "        masks_writer.write(masks_vis)\n",
    "        \n",
    "        frame_count += 1\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Optional: Display frame (comment out for faster processing)\n",
    "        # cv2.imshow('Segmentation', segmentation_frame_bgr)\n",
    "        # cv2.imshow('Masks', masks_vis)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "    \n",
    "    # Clean up\n",
    "    progress_bar.close()\n",
    "    cap.release()\n",
    "    segmentation_writer.release()\n",
    "    masks_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(f\"Processing complete. Processed {frame_count} frames.\")\n",
    "    print(f\"Output videos saved to:\\n- {output_segmentation_path}\\n- {output_masks_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"pottery.mp4\"\n",
    "    output_segmentation = \"pottery_segmentation.mp4\"\n",
    "    output_masks = \"pottery_masks.mp4\"\n",
    "    \n",
    "    # If MPS is causing issues, you can set use_cpu=True to force CPU usage\n",
    "    use_cpu = False\n",
    "    \n",
    "    process_video(input_video, output_segmentation, output_masks, use_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30cfca-d880-4976-bc7d-7a0b907597d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f607ec-2c1b-46cd-8e29-463e6c2c6f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98924fa-7451-43e0-a572-fd9d1b7bc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Ensuring all model parameters are float32 for MPS compatibility\n",
      "Video properties: 1080x1920, 25.0 fps, 84 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   0%|                                                                      | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS float64 issue detected. Falling back to CPU for this frame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   1%|▋                                                          | 1/84 [01:49<2:32:00, 109.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: Generated 31 masks (CPU fallback)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   2%|█▍                                                          | 2/84 [03:17<2:12:22, 96.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: Generated 31 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   4%|██▏                                                         | 3/84 [04:45<2:04:56, 92.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 2: Generated 29 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   5%|██▊                                                         | 4/84 [06:10<1:59:45, 89.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 3: Generated 29 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   6%|███▌                                                        | 5/84 [07:35<1:55:40, 87.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 4: Generated 32 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   7%|████▎                                                       | 6/84 [08:59<1:52:53, 86.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 5: Generated 30 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:   8%|█████                                                       | 7/84 [10:32<1:54:00, 88.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 6: Generated 32 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  10%|█████▋                                                      | 8/84 [12:00<1:52:09, 88.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 7: Generated 30 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  11%|██████▍                                                     | 9/84 [13:27<1:49:50, 87.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 8: Generated 29 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  12%|███████                                                    | 10/84 [14:53<1:47:41, 87.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 9: Generated 33 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  13%|███████▋                                                   | 11/84 [16:26<1:48:34, 89.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 10: Generated 30 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  14%|████████▍                                                  | 12/84 [17:53<1:46:18, 88.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 11: Generated 31 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  15%|█████████▏                                                 | 13/84 [19:21<1:44:32, 88.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 12: Generated 31 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  17%|█████████▊                                                 | 14/84 [20:46<1:41:55, 87.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 13: Generated 33 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  18%|██████████▌                                                | 15/84 [22:12<1:39:57, 86.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 14: Generated 36 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  19%|███████████▏                                               | 16/84 [23:46<1:41:00, 89.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 15: Generated 33 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  20%|███████████▉                                               | 17/84 [25:25<1:42:42, 91.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 16: Generated 34 masks (MPS (GPU))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Acessing frames:  21%|████████████▋                                              | 18/84 [27:04<1:43:28, 94.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 17: Generated 37 masks (MPS (GPU))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# Set default tensor type to float32\n",
    "torch.set_default_dtype(torch.float32)\n",
    "# Ensure numpy arrays are also float32 by default\n",
    "np.set_printoptions(precision=6)\n",
    "\n",
    "# Monkeypatch tensor conversion to prevent float64 on MPS\n",
    "if torch.backends.mps.is_available():\n",
    "    # Override any potential float64 operations to use float32 instead\n",
    "    torch._C._set_float32_matmul_precision('high')\n",
    "    \n",
    "    # Monkeypatch the tensor conversion to intercept float64 conversions\n",
    "    original_tensor_method = torch.Tensor.to\n",
    "    def patched_to_method(self, *args, **kwargs):\n",
    "        # Check if trying to convert to float64\n",
    "        if len(args) > 0 and (args[0] == torch.float64 or \n",
    "                             (isinstance(args[0], torch.dtype) and args[0] == torch.float64)):\n",
    "            print(\"Intercepted attempt to convert tensor to float64, using float32 instead\")\n",
    "            return original_tensor_method(self, torch.float32, *args[1:], **kwargs)\n",
    "        return original_tensor_method(self, *args, **kwargs)\n",
    "    \n",
    "    # Apply the patch\n",
    "    torch.Tensor.to = patched_to_method\n",
    "    \n",
    "    # Also patch the tensor creation functions\n",
    "    original_tensor = torch.tensor\n",
    "    def patched_tensor(*args, **kwargs):\n",
    "        if 'dtype' in kwargs and kwargs['dtype'] == torch.float64:\n",
    "            print(\"Intercepted attempt to create float64 tensor, using float32 instead\")\n",
    "            kwargs['dtype'] = torch.float32\n",
    "        return original_tensor(*args, **kwargs)\n",
    "    \n",
    "    torch.tensor = patched_tensor\n",
    "\n",
    "# Define this as a global variable to use across functions\n",
    "mask_generator = None\n",
    "\n",
    "# Load the SAM model - using MPS for macOS GPU by default\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  # Using MPS for macOS GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_sam_model(device=\"mps\"):\n",
    "    \"\"\"Load the SAM model on the specified device.\"\"\"\n",
    "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "    \n",
    "    # Force model parameters to float32 before moving to device\n",
    "    for param in sam.parameters():\n",
    "        param.data = param.data.to(torch.float32)\n",
    "    \n",
    "    # Move model to the specified device with explicit float32 dtype\n",
    "    sam.to(device=device)\n",
    "    sam.to(dtype=torch.float32)  # Explicitly set dtype after moving to device\n",
    "    \n",
    "    # Force all buffers to float32 as well\n",
    "    for buffer in sam.buffers():\n",
    "        if buffer.data.dtype == torch.float64:\n",
    "            buffer.data = buffer.data.to(torch.float32)\n",
    "    \n",
    "    # Create the mask generator with same parameters as in the notebook\n",
    "    mask_gen = SamAutomaticMaskGenerator(\n",
    "        model=sam,\n",
    "        points_per_side=32,\n",
    "        pred_iou_thresh=0.9,\n",
    "        stability_score_thresh=0.96,\n",
    "        crop_n_layers=1,\n",
    "        crop_n_points_downscale_factor=2,\n",
    "        min_mask_region_area=100,  # Requires open-cv to run post-processing\n",
    "    )\n",
    "    return mask_gen\n",
    "\n",
    "# Initialize global mask generator\n",
    "mask_generator = load_sam_model(device)\n",
    "\n",
    "def apply_masks_to_frame(frame, masks):\n",
    "    \"\"\"Apply colored masks to a frame.\"\"\"\n",
    "    # Create a transparent overlay\n",
    "    mask_overlay = np.ones((frame.shape[0], frame.shape[1], 4), dtype=np.float32)\n",
    "    mask_overlay[:, :, 3] = 0  # Set alpha channel to 0 (transparent)\n",
    "    \n",
    "    # Sort masks by area for better visualization (larger masks behind smaller ones)\n",
    "    sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "    \n",
    "    # Apply each mask with a random color\n",
    "    for mask in sorted_masks:\n",
    "        m = mask['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])  # RGB + alpha\n",
    "        mask_overlay[m] = color_mask\n",
    "    \n",
    "    # Convert frame to RGBA\n",
    "    frame_rgba = np.ones((frame.shape[0], frame.shape[1], 4), dtype=np.float32)\n",
    "    frame_rgba[:, :, :3] = frame / 255.0\n",
    "    frame_rgba[:, :, 3] = 1.0  # Fully opaque\n",
    "    \n",
    "    # Combine the two images (background frame and mask overlay)\n",
    "    composite = frame_rgba * (1 - mask_overlay[:, :, 3:4]) + mask_overlay * mask_overlay[:, :, 3:4]\n",
    "    \n",
    "    # Convert back to uint8 RGB\n",
    "    result = (composite[:, :, :3] * 255).astype(np.uint8)\n",
    "    return result\n",
    "\n",
    "def create_binary_mask_visualization(frame_shape, masks):\n",
    "    \"\"\"Create a visualization where each mask gets a different color.\"\"\"\n",
    "    # Create an empty image\n",
    "    vis = np.zeros((*frame_shape[:2], 3), dtype=np.uint8)\n",
    "    \n",
    "    # Sort masks by area\n",
    "    sorted_masks = sorted(masks, key=(lambda x: x['area']), reverse=True)\n",
    "    \n",
    "    # Assign a random color to each mask\n",
    "    for i, mask in enumerate(sorted_masks):\n",
    "        color = np.random.randint(0, 255, 3, dtype=np.uint8)\n",
    "        vis[mask['segmentation']] = color\n",
    "    \n",
    "    return vis\n",
    "\n",
    "def process_video(input_video_path, output_segmentation_path, output_masks_path, use_cpu=False, frame_skip=0, resize_factor=1.0):\n",
    "    global mask_generator\n",
    "    \n",
    "    # If forcing CPU, reload the model on CPU\n",
    "    if use_cpu and device != \"cpu\":\n",
    "        print(\"Switching to CPU as requested\")\n",
    "        mask_generator = load_sam_model(\"cpu\")\n",
    "    else:\n",
    "        # For MPS, let's try to ensure everything is float32\n",
    "        if device == \"mps\":\n",
    "            print(\"Ensuring all model parameters are float32 for MPS compatibility\")\n",
    "            mask_generator = load_sam_model(\"mps\")\n",
    "    \n",
    "    # Open the input video\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height}, {fps} fps, {total_frames} frames\")\n",
    "    \n",
    "    # Create output video writers\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    segmentation_writer = cv2.VideoWriter(output_segmentation_path, fourcc, fps, (width, height))\n",
    "    masks_writer = cv2.VideoWriter(output_masks_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process each frame\n",
    "    frame_count = 0\n",
    "    processed_count = 0\n",
    "    progress_bar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Optional frame skipping\n",
    "        if frame_skip > 0 and frame_count % (frame_skip + 1) != 0:\n",
    "            frame_count += 1\n",
    "            progress_bar.update(1)\n",
    "            continue\n",
    "        \n",
    "        # Optional resizing for faster processing\n",
    "        if resize_factor != 1.0:\n",
    "            h, w = frame.shape[:2]\n",
    "            new_h, new_w = int(h * resize_factor), int(w * resize_factor)\n",
    "            frame = cv2.resize(frame, (new_w, new_h))\n",
    "        \n",
    "        # Convert from BGR to RGB (SAM expects RGB)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Ensure the frame is in float32 format\n",
    "        frame_rgb = frame_rgb.astype(np.float32) / 255.0\n",
    "        frame_rgb = (frame_rgb * 255.0).astype(np.uint8)\n",
    "        \n",
    "        # Make sure the frame is explicitly set to float32\n",
    "        frame_rgb = frame_rgb.astype(np.float32) / 255.0\n",
    "        frame_rgb = (frame_rgb * 255.0).astype(np.uint8)\n",
    "        \n",
    "        # Generate masks for the current frame\n",
    "        try:\n",
    "            with torch.no_grad():  # Disable gradient tracking for inference\n",
    "                masks = mask_generator.generate(frame_rgb)\n",
    "            device_used = \"MPS (GPU)\"\n",
    "        except TypeError as e:\n",
    "            if \"Cannot convert a MPS Tensor to float64\" in str(e):\n",
    "                # Fall back to CPU if MPS has float64 issues\n",
    "                print(\"MPS float64 issue detected. Falling back to CPU for this frame.\")\n",
    "                # Reload model on CPU with explicit float32\n",
    "                mask_generator = load_sam_model(\"cpu\")\n",
    "                with torch.no_grad():\n",
    "                    masks = mask_generator.generate(frame_rgb)\n",
    "                device_used = \"CPU fallback\"\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_count}: {str(e)}\")\n",
    "            # Try CPU as last resort\n",
    "            mask_generator = load_sam_model(\"cpu\")\n",
    "            with torch.no_grad():\n",
    "                masks = mask_generator.generate(frame_rgb)\n",
    "            device_used = \"CPU fallback (after error)\"\n",
    "            \n",
    "        print(f\"Frame {frame_count}: Generated {len(masks)} masks ({device_used})\")\n",
    "        \n",
    "        # Create visualization with segmentation overlay\n",
    "        segmentation_frame = apply_masks_to_frame(frame_rgb, masks)\n",
    "        segmentation_frame_bgr = cv2.cvtColor(segmentation_frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Create mask visualization\n",
    "        masks_vis = create_binary_mask_visualization(frame.shape, masks)\n",
    "        \n",
    "        # Write to output videos\n",
    "        segmentation_writer.write(segmentation_frame_bgr)\n",
    "        masks_writer.write(masks_vis)\n",
    "        \n",
    "        frame_count += 1\n",
    "        processed_count += 1\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Optionally save a frame periodically to check progress\n",
    "        if processed_count % 10 == 0:\n",
    "            cv2.imwrite(f\"progress_frame_{processed_count}.jpg\", segmentation_frame_bgr)\n",
    "        \n",
    "        # Optional: Display frame (comment out for faster processing)\n",
    "        # cv2.imshow('Segmentation', segmentation_frame_bgr)\n",
    "        # cv2.imshow('Masks', masks_vis)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "    \n",
    "    # Clean up\n",
    "    progress_bar.close()\n",
    "    cap.release()\n",
    "    segmentation_writer.release()\n",
    "    masks_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(f\"Processing complete. Processed {processed_count}/{frame_count} frames.\")\n",
    "    print(f\"Output videos saved to:\\n- {output_segmentation_path}\\n- {output_masks_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_video = \"pottery.mp4\"\n",
    "    output_segmentation = \"pottery_segmentation.mp4\"\n",
    "    output_masks = \"pottery_masks.mp4\"\n",
    "    \n",
    "    # If MPS is causing issues, you can set use_cpu=True to force CPU usage\n",
    "    use_cpu = False\n",
    "    \n",
    "    # Optional: Skip frames to speed up processing (0 = process every frame)\n",
    "    frame_skip = 0\n",
    "    \n",
    "    # Optional: Resize factor to reduce processing time (1.0 = original size)\n",
    "    # For example, 0.5 will reduce the width and height by half, processing 4x faster\n",
    "    resize_factor = 0.5\n",
    "    \n",
    "    process_video(input_video, output_segmentation, output_masks, \n",
    "                 use_cpu=use_cpu, frame_skip=frame_skip, resize_factor=resize_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf719fa1-9b2e-4524-8db8-45c504d051f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:segmentanything] *",
   "language": "python",
   "name": "conda-env-segmentanything-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
