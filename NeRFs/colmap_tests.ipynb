{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7c4f5a-ea67-4177-a10b-7b771f99f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S'està clonant a «LLFF»...\n",
      "remote: Enumerating objects: 774, done.\u001b[K\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
      "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
      "remote: Total 774 (delta 7), reused 20 (delta 5), pack-reused 748 (from 1)\u001b[K\n",
      "S'estan rebent objectes: 100% (774/774), 31.94 MiB | 4.75 MiB/s, fet.\n",
      "S'estan resolent les diferències: 100% (409/409), fet.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Fyusion/LLFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0edee44f-d2ec-445c-82fe-4ec3556989f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to run COLMAP\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jorgemuyo/Desktop/CraftReplicator/imgs2poses.py\", line 18, in <module>\n",
      "    gen_poses(args.scenedir, args.match_type)\n",
      "  File \"/Users/jorgemuyo/Desktop/CraftReplicator/llff/poses/pose_utils.py\", line 268, in gen_poses\n",
      "    run_colmap(basedir, match_type)\n",
      "  File \"/Users/jorgemuyo/Desktop/CraftReplicator/llff/poses/colmap_wrapper.py\", line 26, in run_colmap\n",
      "    logfile = open(logfile_name, 'w')\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/fox/colmap_output.txt'\n"
     ]
    }
   ],
   "source": [
    "# change the path below to your data folder (the folder containing the `images` folder)\n",
    "!python3 imgs2poses.py \"/fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df1af9-8d1c-468c-b03e-cc688b354f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ad62e-4f8a-436d-9268-dd457bd2374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c67b8-5ee8-41e5-abb1-4abc2de34efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5922e325-8976-4a36-88aa-908cbe4d3953",
   "metadata": {},
   "source": [
    "# Camera / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e62ab8-9524-48b6-a646-01b27c9a3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "o, d, target_px_values = get_rays('fox', mode='train')\n",
    "dataloader = DataLoader(torch.cat((torch.from_numpy(o).reshape(-1, 3).type(torch.float),\n",
    "                                   torch.from_numpy(d).reshape(-1, 3).type(torch.float),\n",
    "                                   torch.from_numpy(target_px_values).reshape(-1, 3).type(torch.float)), dim=1),\n",
    "                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "dataloader_warmup = DataLoader(torch.cat((torch.from_numpy(o).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(d).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(target_px_values).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float)), dim=1),\n",
    "                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_o, test_d, test_target_px_values = get_rays('fox', mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc6284-ff38-4a9d-a914-7cd8ff9163fb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd96eec-e258-469d-8781-da1dc2bd1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device\n",
    "\n",
    "tn = 8.\n",
    "tf = 12.\n",
    "nb_epochs = 1 #15 30\n",
    "lr =  1e-3 # 1e-3 5e-4\n",
    "gamma = .5 #0.5 0.7 \n",
    "nb_bins = 100 #100 256\n",
    "\n",
    "model = Nerf(hidden_dim=256).to(device) #Nerf(hidden_dim=128).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10], gamma=gamma)\n",
    "\n",
    "\n",
    "\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, 1, dataloader_warmup, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, nb_epochs, dataloader, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2ceae-86b6-4d6d-aafe-ac3627807f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_nerf_colmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be732b56-fa20-4a47-8b2a-f918c1ea5d7a",
   "metadata": {},
   "source": [
    "# Mesh extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6764d29-c8b9-417f-ad7f-a2aa42554303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/_x9hjmys03x5gnbfl70ry2sr0000gn/T/ipykernel_30530/505662932.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('nerf_model.pth')\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('nerf_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3fcae-cfbe-45cb-a1cb-2bc9dc639f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeRF()\n",
    "\n",
    "# Load the state dict\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca056f-722f-4780-a5df-61a5b7f2dfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e0f3d9-71c0-4149-93a3-a16ff6263e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating density volume with resolution 700...\n",
      "Sampling density field...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnerf_mesh.obj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Can also use .obj format\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Extract and save the mesh\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m mesh \u001b[38;5;241m=\u001b[39m \u001b[43msave_colored_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 121\u001b[0m, in \u001b[0;36msave_colored_mesh\u001b[0;34m(nerf_model, output_path, resolution, threshold, device)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_colored_mesh\u001b[39m(nerf_model, output_path, resolution\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Extract and save a colored mesh from a NeRF model.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m        device: Torch device to use\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m \u001b[43mextract_mesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnerf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing mesh...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# Optional mesh cleanup\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 58\u001b[0m, in \u001b[0;36mextract_mesh\u001b[0;34m(nerf_model, resolution, threshold, bbox_min, bbox_max, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m         chunk_points \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)[i:i\u001b[38;5;241m+\u001b[39mchunk_size]\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Assume model returns (rgb, sigma) tuple\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m         _, chunk_densities \u001b[38;5;241m=\u001b[39m \u001b[43mnerf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_points\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m         density_volume\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[i:i\u001b[38;5;241m+\u001b[39mchunk_size] \u001b[38;5;241m=\u001b[39m chunk_densities\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Auto-determine threshold if not provided\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_density_field(density_volume):\n",
    "    \"\"\"Analyze the density field to help choose a good threshold.\"\"\"\n",
    "    min_density = float(density_volume.min())\n",
    "    max_density = float(density_volume.max())\n",
    "    mean_density = float(density_volume.mean())\n",
    "    std_density = float(density_volume.std())\n",
    "    \n",
    "    print(f\"Density field statistics:\")\n",
    "    print(f\"Min: {min_density:.6f}\")\n",
    "    print(f\"Max: {max_density:.6f}\")\n",
    "    print(f\"Mean: {mean_density:.6f}\")\n",
    "    print(f\"Std: {std_density:.6f}\")\n",
    "    \n",
    "    # Suggest threshold as mean + 1 std deviation\n",
    "    suggested_threshold = mean_density + std_density\n",
    "    return suggested_threshold\n",
    "\n",
    "def extract_mesh(nerf_model, resolution=128, threshold=None, bbox_min=[-1.5, -1.5, -1.5], \n",
    "                bbox_max=[1.5, 1.5, 1.5], device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Extract a colored mesh from a trained NeRF model.\n",
    "    \n",
    "    Args:\n",
    "        nerf_model: Trained NeRF model\n",
    "        resolution: Grid resolution for marching cubes\n",
    "        threshold: Density threshold for surface extraction (if None, will be auto-determined)\n",
    "        bbox_min: Minimum corner of bounding box\n",
    "        bbox_max: Maximum corner of bounding box\n",
    "        device: Torch device to use\n",
    "    \n",
    "    Returns:\n",
    "        trimesh.Trimesh: Colored mesh\n",
    "    \"\"\"\n",
    "    print(f\"Creating density volume with resolution {resolution}...\")\n",
    "    \n",
    "    # Create grid of points\n",
    "    x = torch.linspace(bbox_min[0], bbox_max[0], resolution)\n",
    "    y = torch.linspace(bbox_min[1], bbox_max[1], resolution)\n",
    "    z = torch.linspace(bbox_min[2], bbox_max[2], resolution)\n",
    "    xx, yy, zz = torch.meshgrid(x, y, z, indexing='ij')\n",
    "    points = torch.stack([xx, yy, zz], dim=-1).to(device)\n",
    "    \n",
    "    # Create density volume\n",
    "    density_volume = torch.zeros((resolution, resolution, resolution))\n",
    "    chunk_size = 512 * 512  # Process in chunks to avoid OOM\n",
    "    \n",
    "    print(\"Sampling density field...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, points.numel() // 3, chunk_size):\n",
    "            chunk_points = points.reshape(-1, 3)[i:i+chunk_size]\n",
    "            # Assume model returns (rgb, sigma) tuple\n",
    "            _, chunk_densities = nerf_model(chunk_points, torch.zeros_like(chunk_points))\n",
    "            density_volume.reshape(-1)[i:i+chunk_size] = chunk_densities.cpu()\n",
    "    \n",
    "    # Auto-determine threshold if not provided\n",
    "    if threshold is None:\n",
    "        threshold = analyze_density_field(density_volume)\n",
    "        print(f\"Auto-determined threshold: {threshold:.6f}\")\n",
    "    \n",
    "    print(f\"Extracting mesh with threshold {threshold}...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract mesh using marching cubes\n",
    "        vertices, faces, normals, _ = marching_cubes(\n",
    "            density_volume.numpy(),\n",
    "            threshold,\n",
    "            spacing=((bbox_max[0] - bbox_min[0])/resolution,\n",
    "                    (bbox_max[1] - bbox_min[1])/resolution,\n",
    "                    (bbox_max[2] - bbox_min[2])/resolution)\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\"Error during marching cubes:\")\n",
    "        print(e)\n",
    "        print(\"\\nTry adjusting the threshold based on the density statistics above.\")\n",
    "        raise\n",
    "    \n",
    "    print(f\"Mesh extracted with {len(vertices)} vertices and {len(faces)} faces\")\n",
    "    \n",
    "    # Adjust vertices to match bbox\n",
    "    vertices = vertices + np.array(bbox_min)\n",
    "    \n",
    "    # Sample colors at vertex positions\n",
    "    vertex_colors = torch.zeros((len(vertices), 3))\n",
    "    vertices_tensor = torch.tensor(vertices, dtype=torch.float32).to(device)\n",
    "    \n",
    "    print(\"Sampling colors...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(vertices), chunk_size):\n",
    "            chunk_vertices = vertices_tensor[i:i+chunk_size]\n",
    "            # Assume model returns (rgb, sigma) tuple\n",
    "            chunk_colors, _ = nerf_model(chunk_vertices, torch.zeros_like(chunk_vertices))\n",
    "            vertex_colors[i:i+chunk_size] = chunk_colors.cpu()\n",
    "    \n",
    "    # Create mesh with vertex colors\n",
    "    mesh = trimesh.Trimesh(\n",
    "        vertices=vertices,\n",
    "        faces=faces,\n",
    "        vertex_colors=(vertex_colors.numpy() * 255).astype(np.uint8),\n",
    "        vertex_normals=normals\n",
    "    )\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "def save_colored_mesh(nerf_model, output_path, resolution=256, threshold=None, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Extract and save a colored mesh from a NeRF model.\n",
    "    \n",
    "    Args:\n",
    "        nerf_model: Trained NeRF model\n",
    "        output_path: Path to save the mesh (should end in .ply or .obj)\n",
    "        resolution: Resolution for marching cubes\n",
    "        threshold: Density threshold (if None, will be auto-determined)\n",
    "        device: Torch device to use\n",
    "    \"\"\"\n",
    "    mesh = extract_mesh(nerf_model, resolution=resolution, threshold=threshold, device=device)\n",
    "    \n",
    "    print(\"Processing mesh...\")\n",
    "    # Optional mesh cleanup\n",
    "    mesh = mesh.process(validate=True)\n",
    "    \n",
    "    print(f\"Saving mesh to {output_path}...\")\n",
    "    # Save the mesh\n",
    "    mesh.export(output_path)\n",
    "    return mesh\n",
    "\n",
    "# After loading your model\n",
    "resolution = 700  # Increase for better quality, decrease if you run into memory issues\n",
    "output_path = \"nerf_mesh.obj\"  # Can also use .obj format\n",
    "\n",
    "# Extract and save the mesh\n",
    "mesh = save_colored_mesh(model, output_path, resolution=resolution, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b528771-b4d7-4285-8fd3-d15842b3f8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e15e1-1f6b-4372-9ee0-22a25cea75c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d3a25-9d9d-4248-a210-9876f24a5f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2956aa-8e87-44fc-94d9-8b3fd4c54569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91e1c2-ce41-4977-a3f9-944067a3af11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b0840-5978-4b21-b1b0-07dc04daa80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99cb50-6a3a-482f-9e91-493359805a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ece44-7ef1-4086-ac35-0054a0d37394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Pymcubes\n",
    "#!pip install trimesh\n",
    "#!pip install -U scikit-image\n",
    "#!pip install genesis-world  # Requires Python >=3.9;\n",
    "#!pip uninstall genesis-world\n",
    "#!conda install -c anaconda trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1340391-1311-4703-87db-6b37b442b4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac8b37-99bb-41b7-92e3-71ef9d9f9e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c97ed5-c343-4e17-957a-137d1c1a1acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de10ac4-4564-416c-81f5-0c6d22286d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newenv] *",
   "language": "python",
   "name": "conda-env-newenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
