{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257860de-71ad-4386-88bd-fc945e3bc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#from dataset import get_rays\n",
    "from rendering import rendering\n",
    "from model import Voxels, Nerf\n",
    "from ml_helpers import training\n",
    "\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "import trimesh.smoothing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edee44f-d2ec-445c-82fe-4ec3556989f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.5.1\n",
      "device:  mps\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version: \", torch.__version__)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2092d3b-8113-45c3-94a6-6fc79a1e23ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import struct\n",
    "\n",
    "def read_next_bytes(fid, num_bytes, format_char_sequence, endian_character=\"<\"):\n",
    "    data = fid.read(num_bytes)\n",
    "    if not data:\n",
    "        return None\n",
    "    try:\n",
    "        return struct.unpack(endian_character + format_char_sequence, data)\n",
    "    except struct.error:\n",
    "        return None\n",
    "\n",
    "def read_cameras_binary(path_to_model_file):\n",
    "    cameras = {}\n",
    "    with open(path_to_model_file, \"rb\") as fid:\n",
    "        num_cameras = read_next_bytes(fid, 8, \"Q\")[0]\n",
    "        for _ in range(num_cameras):\n",
    "            try:\n",
    "                camera_properties = read_next_bytes(fid, 24, \"iiQQ\")\n",
    "                camera_id = camera_properties[0]\n",
    "                width = camera_properties[2]\n",
    "                height = camera_properties[3]\n",
    "                \n",
    "                num_params = read_next_bytes(fid, 8, \"Q\")[0]\n",
    "                if num_params > 1000:  # Sanity check\n",
    "                    continue\n",
    "                    \n",
    "                params = read_next_bytes(fid, 8*num_params, \"d\" * num_params)\n",
    "                if params is not None:\n",
    "                    cameras[camera_id] = {\n",
    "                        \"width\": width,\n",
    "                        \"height\": height,\n",
    "                        \"params\": params\n",
    "                    }\n",
    "            except:\n",
    "                continue\n",
    "    return cameras\n",
    "\n",
    "def read_images_binary(path_to_model_file, chunk_size=100):\n",
    "    images = {}\n",
    "    with open(path_to_model_file, \"rb\") as fid:\n",
    "        num_reg_images = read_next_bytes(fid, 8, \"Q\")[0]\n",
    "        for img_index in range(num_reg_images):\n",
    "            try:\n",
    "                binary_image_properties = read_next_bytes(fid, 64, \"idddddddi\")\n",
    "                if binary_image_properties is None:\n",
    "                    continue\n",
    "                    \n",
    "                image_id = binary_image_properties[0]\n",
    "                qvec = np.array(binary_image_properties[1:5], dtype=np.float32)\n",
    "                tvec = np.array(binary_image_properties[5:8], dtype=np.float32)\n",
    "                camera_id = binary_image_properties[8]\n",
    "\n",
    "                # Read image name\n",
    "                image_name = \"\"\n",
    "                while True:\n",
    "                    current_char = read_next_bytes(fid, 1, \"c\")\n",
    "                    if current_char is None or current_char[0] == b\"\\x00\":\n",
    "                        break\n",
    "                    image_name += current_char[0].decode(\"utf-8\")\n",
    "                \n",
    "                # Skip points2D\n",
    "                num_points2D = read_next_bytes(fid, 8, \"Q\")\n",
    "                if num_points2D is None:\n",
    "                    continue\n",
    "                fid.seek(24*num_points2D[0], 1)\n",
    "                \n",
    "                images[image_id] = {\n",
    "                    \"qvec\": qvec,\n",
    "                    \"tvec\": tvec,\n",
    "                    \"camera_id\": camera_id,\n",
    "                    \"name\": image_name,\n",
    "                }\n",
    "                \n",
    "                if len(images) >= chunk_size:\n",
    "                    yield images\n",
    "                    images = {}\n",
    "                    \n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if images:\n",
    "            yield images\n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "    return np.array([\n",
    "        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "         2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],\n",
    "        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "         1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
    "         2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],\n",
    "        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "         2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "         1 - 2 * qvec[1]**2 - 2 * qvec[2]**2]], dtype=np.float32)\n",
    "\n",
    "def process_image_batch(image_batch, cameras, datapath, batch_size=2):\n",
    "    print(f\"Processing {len(image_batch)} images\")\n",
    "    print(f\"First image path: {os.path.join(datapath, image_batch[0][1]['name'])}\")\n",
    "    for idx in range(0, len(image_batch), batch_size):\n",
    "        batch = image_batch[idx:idx + batch_size]\n",
    "        batch_rays_o = []\n",
    "        batch_rays_d = []\n",
    "        batch_pixels = []\n",
    "        \n",
    "        for image_id, image_data in batch:\n",
    "            try:\n",
    "                camera = cameras[image_data['camera_id']]\n",
    "                H, W = camera['height'], camera['width']\n",
    "                f = float(camera['params'][0])\n",
    "                \n",
    "                img_path = os.path.join(datapath, image_data['name'])\n",
    "                img = imageio.imread(img_path).astype(np.float32) / 255.0\n",
    "                if img.shape[-1] == 4:\n",
    "                    img = img[..., :3] * img[..., -1:] + (1 - img[..., -1:])\n",
    "                \n",
    "                R = qvec2rotmat(image_data['qvec'])\n",
    "                t = image_data['tvec']\n",
    "                C = -R.T @ t\n",
    "                \n",
    "                u, v = np.meshgrid(np.arange(W), np.arange(H))\n",
    "                x = (u - W/2).reshape(-1)\n",
    "                y = -(v - H/2).reshape(-1)\n",
    "                z = -np.ones_like(x) * f\n",
    "                \n",
    "                dirs = np.stack([x, y, z], axis=-1)\n",
    "                dirs = (R.T @ dirs.T).T\n",
    "                dirs = dirs / np.linalg.norm(dirs, axis=-1, keepdims=True)\n",
    "                \n",
    "                batch_rays_o.append(np.broadcast_to(C, (H*W, 3)))\n",
    "                batch_rays_d.append(dirs)\n",
    "                batch_pixels.append(img.reshape(-1, 3))\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        if batch_rays_o:\n",
    "            yield (np.stack(batch_rays_o),\n",
    "                  np.stack(batch_rays_d),\n",
    "                  np.stack(batch_pixels))\n",
    "\n",
    "def get_rays(datapath, mode='train', split_ratio=0.9):\n",
    "    sparse_dir = os.path.join(datapath, 'colmap', 'sparse', '0')\n",
    "    \n",
    "    # Read cameras data\n",
    "    cameras = read_cameras_binary(os.path.join(sparse_dir, 'cameras.bin'))\n",
    "    if not cameras:\n",
    "        raise ValueError(\"No cameras found in the binary file\")\n",
    "\n",
    "    # Process images in chunks\n",
    "    image_chunks = list(read_images_binary(os.path.join(sparse_dir, 'images.bin')))\n",
    "    if not image_chunks:\n",
    "        raise ValueError(\"No images found in the binary file\")\n",
    "        \n",
    "    # Merge chunks and sort\n",
    "    all_images = {}\n",
    "    for chunk in image_chunks:\n",
    "        all_images.update(chunk)\n",
    "    image_list = sorted(all_images.items(), key=lambda x: x[1]['name'])\n",
    "    \n",
    "    # Split train/test\n",
    "    split_idx = int(len(image_list) * split_ratio)\n",
    "    if mode == 'train':\n",
    "        image_list = image_list[:split_idx]\n",
    "    else:\n",
    "        image_list = image_list[split_idx:]\n",
    "        \n",
    "    # Process images in batches\n",
    "    all_rays_o = []\n",
    "    all_rays_d = []\n",
    "    all_pixels = []\n",
    "    \n",
    "    for rays_o, rays_d, pixels in process_image_batch(image_list, cameras, datapath):\n",
    "        all_rays_o.append(rays_o)\n",
    "        all_rays_d.append(rays_d)\n",
    "        all_pixels.append(pixels)\n",
    "        \n",
    "    if not all_rays_o:\n",
    "        raise ValueError(\"No valid images processed\")\n",
    "        \n",
    "    rays_o = np.concatenate(all_rays_o)\n",
    "    rays_d = np.concatenate(all_rays_d)\n",
    "    pixels = np.concatenate(all_pixels)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        # Get dimensions from camera\n",
    "        H = cameras[image_list[0][1]['camera_id']]['height']\n",
    "        W = cameras[image_list[0][1]['camera_id']]['width']\n",
    "        rays_o = rays_o.reshape(-1, H, W, 3)\n",
    "        rays_d = rays_d.reshape(-1, H, W, 3)\n",
    "        pixels = pixels.reshape(-1, H, W, 3)\n",
    "    \n",
    "    return rays_o, rays_d, pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922e325-8976-4a36-88aa-908cbe4d3953",
   "metadata": {},
   "source": [
    "# Camera / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e62ab8-9524-48b6-a646-01b27c9a3fd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid images processed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m----> 2\u001b[0m o, d, target_px_values \u001b[38;5;241m=\u001b[39m \u001b[43mget_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfox\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Main dataloader\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39mfrom_numpy(o)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m      6\u001b[0m                                torch\u001b[38;5;241m.\u001b[39mfrom_numpy(d)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m      7\u001b[0m                                torch\u001b[38;5;241m.\u001b[39mfrom_numpy(target_px_values)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      8\u001b[0m                        batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[3], line 179\u001b[0m, in \u001b[0;36mget_rays\u001b[0;34m(datapath, mode, split_ratio)\u001b[0m\n\u001b[1;32m    176\u001b[0m     all_pixels\u001b[38;5;241m.\u001b[39mappend(pixels)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_rays_o:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid images processed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    181\u001b[0m rays_o \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_rays_o)\n\u001b[1;32m    182\u001b[0m rays_d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_rays_d)\n",
      "\u001b[0;31mValueError\u001b[0m: No valid images processed"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "o, d, target_px_values = get_rays('fox', mode='train')\n",
    "\n",
    "# Main dataloader\n",
    "dataloader = DataLoader(torch.cat((torch.from_numpy(o).reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(d).reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(target_px_values).reshape(-1, 3).type(torch.float)), dim=1),\n",
    "                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Warmup dataloader with center crop\n",
    "o_warmup = o[:, 100:300, 100:300, :].reshape(-1, 3)\n",
    "d_warmup = d[:, 100:300, 100:300, :].reshape(-1, 3)\n",
    "target_warmup = target_px_values[:, 100:300, 100:300, :].reshape(-1, 3)\n",
    "\n",
    "dataloader_warmup = DataLoader(torch.cat((torch.from_numpy(o_warmup).type(torch.float),\n",
    "                           torch.from_numpy(d_warmup).type(torch.float),\n",
    "                           torch.from_numpy(target_warmup).type(torch.float)), dim=1),\n",
    "                   batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_o, test_d, test_target_px_values = get_rays('fox', mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc6284-ff38-4a9d-a914-7cd8ff9163fb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd96eec-e258-469d-8781-da1dc2bd1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device\n",
    "\n",
    "tn = 8.\n",
    "tf = 12.\n",
    "nb_epochs = 1 #15 30\n",
    "lr =  1e-3 # 1e-3 5e-4\n",
    "gamma = .5 #0.5 0.7 \n",
    "nb_bins = 100 #100 256\n",
    "\n",
    "model = Nerf(hidden_dim=256).to(device) #Nerf(hidden_dim=128).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10], gamma=gamma)\n",
    "\n",
    "\n",
    "\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, 1, dataloader_warmup, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, nb_epochs, dataloader, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2ceae-86b6-4d6d-aafe-ac3627807f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_nerf_colmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be732b56-fa20-4a47-8b2a-f918c1ea5d7a",
   "metadata": {},
   "source": [
    "# Mesh extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6764d29-c8b9-417f-ad7f-a2aa42554303",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_nerf_colmap').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0f3d9-71c0-4149-93a3-a16ff6263e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def analyze_density_field(density_volume):\n",
    "    \"\"\"Analyze the density field to help choose a good threshold.\"\"\"\n",
    "    min_density = float(density_volume.min())\n",
    "    max_density = float(density_volume.max())\n",
    "    mean_density = float(density_volume.mean())\n",
    "    std_density = float(density_volume.std())\n",
    "    \n",
    "    print(f\"Density field statistics:\")\n",
    "    print(f\"Min: {min_density:.6f}\")\n",
    "    print(f\"Max: {max_density:.6f}\")\n",
    "    print(f\"Mean: {mean_density:.6f}\")\n",
    "    print(f\"Std: {std_density:.6f}\")\n",
    "    \n",
    "    # Suggest threshold as mean + 1 std deviation\n",
    "    suggested_threshold = mean_density + std_density\n",
    "    return suggested_threshold\n",
    "\n",
    "def extract_mesh(nerf_model, resolution=128, threshold=None, bbox_min=[-1.5, -1.5, -1.5], \n",
    "                bbox_max=[1.5, 1.5, 1.5], device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Extract a colored mesh from a trained NeRF model.\n",
    "    \n",
    "    Args:\n",
    "        nerf_model: Trained NeRF model\n",
    "        resolution: Grid resolution for marching cubes\n",
    "        threshold: Density threshold for surface extraction (if None, will be auto-determined)\n",
    "        bbox_min: Minimum corner of bounding box\n",
    "        bbox_max: Maximum corner of bounding box\n",
    "        device: Torch device to use\n",
    "    \n",
    "    Returns:\n",
    "        trimesh.Trimesh: Colored mesh\n",
    "    \"\"\"\n",
    "    print(f\"Creating density volume with resolution {resolution}...\")\n",
    "    \n",
    "    # Create grid of points\n",
    "    x = torch.linspace(bbox_min[0], bbox_max[0], resolution)\n",
    "    y = torch.linspace(bbox_min[1], bbox_max[1], resolution)\n",
    "    z = torch.linspace(bbox_min[2], bbox_max[2], resolution)\n",
    "    xx, yy, zz = torch.meshgrid(x, y, z, indexing='ij')\n",
    "    points = torch.stack([xx, yy, zz], dim=-1).to(device)\n",
    "    \n",
    "    # Create density volume\n",
    "    density_volume = torch.zeros((resolution, resolution, resolution))\n",
    "    chunk_size = 512 * 512  # Process in chunks to avoid OOM\n",
    "    \n",
    "    print(\"Sampling density field...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, points.numel() // 3, chunk_size):\n",
    "            chunk_points = points.reshape(-1, 3)[i:i+chunk_size]\n",
    "            # Assume model returns (rgb, sigma) tuple\n",
    "            _, chunk_densities = nerf_model(chunk_points, torch.zeros_like(chunk_points))\n",
    "            density_volume.reshape(-1)[i:i+chunk_size] = chunk_densities.cpu()\n",
    "    \n",
    "    # Auto-determine threshold if not provided\n",
    "    if threshold is None:\n",
    "        threshold = analyze_density_field(density_volume)\n",
    "        print(f\"Auto-determined threshold: {threshold:.6f}\")\n",
    "    \n",
    "    print(f\"Extracting mesh with threshold {threshold}...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract mesh using marching cubes\n",
    "        vertices, faces, normals, _ = marching_cubes(\n",
    "            density_volume.numpy(),\n",
    "            threshold,\n",
    "            spacing=((bbox_max[0] - bbox_min[0])/resolution,\n",
    "                    (bbox_max[1] - bbox_min[1])/resolution,\n",
    "                    (bbox_max[2] - bbox_min[2])/resolution)\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(\"Error during marching cubes:\")\n",
    "        print(e)\n",
    "        print(\"\\nTry adjusting the threshold based on the density statistics above.\")\n",
    "        raise\n",
    "    \n",
    "    print(f\"Mesh extracted with {len(vertices)} vertices and {len(faces)} faces\")\n",
    "    \n",
    "    # Adjust vertices to match bbox\n",
    "    vertices = vertices + np.array(bbox_min)\n",
    "    \n",
    "    # Sample colors at vertex positions\n",
    "    vertex_colors = torch.zeros((len(vertices), 3))\n",
    "    vertices_tensor = torch.tensor(vertices, dtype=torch.float32).to(device)\n",
    "    \n",
    "    print(\"Sampling colors...\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(vertices), chunk_size):\n",
    "            chunk_vertices = vertices_tensor[i:i+chunk_size]\n",
    "            # Assume model returns (rgb, sigma) tuple\n",
    "            chunk_colors, _ = nerf_model(chunk_vertices, torch.zeros_like(chunk_vertices))\n",
    "            vertex_colors[i:i+chunk_size] = chunk_colors.cpu()\n",
    "    \n",
    "    # Create mesh with vertex colors\n",
    "    mesh = trimesh.Trimesh(\n",
    "        vertices=vertices,\n",
    "        faces=faces,\n",
    "        vertex_colors=(vertex_colors.numpy() * 255).astype(np.uint8),\n",
    "        vertex_normals=normals\n",
    "    )\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "def save_colored_mesh(nerf_model, output_path, resolution=256, threshold=None, device=torch.device(\"cpu\")):\n",
    "    \"\"\"\n",
    "    Extract and save a colored mesh from a NeRF model.\n",
    "    \n",
    "    Args:\n",
    "        nerf_model: Trained NeRF model\n",
    "        output_path: Path to save the mesh (should end in .ply or .obj)\n",
    "        resolution: Resolution for marching cubes\n",
    "        threshold: Density threshold (if None, will be auto-determined)\n",
    "        device: Torch device to use\n",
    "    \"\"\"\n",
    "    mesh = extract_mesh(nerf_model, resolution=resolution, threshold=threshold, device=device)\n",
    "    \n",
    "    print(\"Processing mesh...\")\n",
    "    # Optional mesh cleanup\n",
    "    mesh = mesh.process(validate=True)\n",
    "    \n",
    "    print(f\"Saving mesh to {output_path}...\")\n",
    "    # Save the mesh\n",
    "    mesh.export(output_path)\n",
    "    return mesh\n",
    "\n",
    "# After loading your model\n",
    "resolution = 700  # Increase for better quality, decrease if you run into memory issues\n",
    "output_path = \"nerf_mesh.obj\"  # Can also use .obj format\n",
    "\n",
    "# Extract and save the mesh\n",
    "mesh = save_colored_mesh(model, output_path, resolution=resolution, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b528771-b4d7-4285-8fd3-d15842b3f8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e15e1-1f6b-4372-9ee0-22a25cea75c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d3a25-9d9d-4248-a210-9876f24a5f80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2956aa-8e87-44fc-94d9-8b3fd4c54569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91e1c2-ce41-4977-a3f9-944067a3af11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b0840-5978-4b21-b1b0-07dc04daa80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99cb50-6a3a-482f-9e91-493359805a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ece44-7ef1-4086-ac35-0054a0d37394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Pymcubes\n",
    "#!pip install trimesh\n",
    "#!pip install -U scikit-image\n",
    "#!pip install genesis-world  # Requires Python >=3.9;\n",
    "#!pip uninstall genesis-world\n",
    "#!conda install -c anaconda trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1340391-1311-4703-87db-6b37b442b4c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac8b37-99bb-41b7-92e3-71ef9d9f9e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c97ed5-c343-4e17-957a-137d1c1a1acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de10ac4-4564-416c-81f5-0c6d22286d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newenv] *",
   "language": "python",
   "name": "conda-env-newenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
