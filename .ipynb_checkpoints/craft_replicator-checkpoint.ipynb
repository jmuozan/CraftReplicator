{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257860de-71ad-4386-88bd-fc945e3bc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import get_rays\n",
    "from rendering import rendering\n",
    "from model import Voxels, Nerf\n",
    "from ml_helpers import training\n",
    "\n",
    "from skimage.measure import marching_cubes\n",
    "import trimesh\n",
    "import trimesh.smoothing\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edee44f-d2ec-445c-82fe-4ec3556989f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.5.1\n",
      "device:  mps\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version: \", torch.__version__)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922e325-8976-4a36-88aa-908cbe4d3953",
   "metadata": {},
   "source": [
    "# Camera / Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e62ab8-9524-48b6-a646-01b27c9a3fd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m----> 3\u001b[0m o, d, target_px_values \u001b[38;5;241m=\u001b[39m \u001b[43mget_rays\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfox\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39mfrom_numpy(o)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m      5\u001b[0m                                    torch\u001b[38;5;241m.\u001b[39mfrom_numpy(d)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m      6\u001b[0m                                    torch\u001b[38;5;241m.\u001b[39mfrom_numpy(target_px_values)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      7\u001b[0m                        batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m dataloader_warmup \u001b[38;5;241m=\u001b[39m DataLoader(torch\u001b[38;5;241m.\u001b[39mcat((torch\u001b[38;5;241m.\u001b[39mfrom_numpy(o)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m3\u001b[39m)[:, \u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m300\u001b[39m, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m     11\u001b[0m                                torch\u001b[38;5;241m.\u001b[39mfrom_numpy(d)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m3\u001b[39m)[:, \u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m300\u001b[39m, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat),\n\u001b[1;32m     12\u001b[0m                                torch\u001b[38;5;241m.\u001b[39mfrom_numpy(target_px_values)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m3\u001b[39m)[:, \u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m100\u001b[39m:\u001b[38;5;241m300\u001b[39m, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     13\u001b[0m                        batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/CraftReplicator/dataset.py:15\u001b[0m, in \u001b[0;36mget_rays\u001b[0;34m(datapath, mode)\u001b[0m\n\u001b[1;32m     12\u001b[0m img_file_names \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(datapath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/imgs\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pose_file_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(intrisics_file_names)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(img_file_names) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(pose_file_names)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Read\u001b[39;00m\n\u001b[1;32m     18\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pose_file_names)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "o, d, target_px_values = get_rays('fox', mode='train')\n",
    "dataloader = DataLoader(torch.cat((torch.from_numpy(o).reshape(-1, 3).type(torch.float),\n",
    "                                   torch.from_numpy(d).reshape(-1, 3).type(torch.float),\n",
    "                                   torch.from_numpy(target_px_values).reshape(-1, 3).type(torch.float)), dim=1),\n",
    "                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "dataloader_warmup = DataLoader(torch.cat((torch.from_numpy(o).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(d).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float),\n",
    "                               torch.from_numpy(target_px_values).reshape(90, 400, 400, 3)[:, 100:300, 100:300, :].reshape(-1, 3).type(torch.float)), dim=1),\n",
    "                       batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_o, test_d, test_target_px_values = get_rays('fox', mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc6284-ff38-4a9d-a914-7cd8ff9163fb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd96eec-e258-469d-8781-da1dc2bd1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device\n",
    "\n",
    "tn = 8.\n",
    "tf = 12.\n",
    "nb_epochs = 30 #15\n",
    "lr = 5e-4 #1e-3\n",
    "gamma = 0.7 #.5\n",
    "nb_bins = 256 #100\n",
    "\n",
    "model = Nerf(hidden_dim=256).to(device) #Nerf(hidden_dim=128).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 10], gamma=gamma)\n",
    "\n",
    "\n",
    "\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, 1, dataloader_warmup, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()\n",
    "training_loss = training(model, optimizer, scheduler, tn, tf, nb_bins, nb_epochs, dataloader, device=device)\n",
    "plt.plot(training_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2ceae-86b6-4d6d-aafe-ac3627807f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_nerf_mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be732b56-fa20-4a47-8b2a-f918c1ea5d7a",
   "metadata": {},
   "source": [
    "# Mesh extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6764d29-c8b9-417f-ad7f-a2aa42554303",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model_nerf_mps').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ece44-7ef1-4086-ac35-0054a0d37394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Pymcubes\n",
    "#!pip install trimesh\n",
    "#!pip install -U scikit-image\n",
    "#!pip install genesis-world  # Requires Python >=3.9;\n",
    "#!pip uninstall genesis-world\n",
    "#!conda install -c anaconda trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1340391-1311-4703-87db-6b37b442b4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colored_mesh(model, device, N=200, scale=1.5, batch_size=50000, density_threshold=None):\n",
    "    \"\"\"Generate a detailed mesh from a NeRF model with vertex colors.\"\"\"\n",
    "    # Generate grid points\n",
    "    x = torch.linspace(-scale, scale, N)\n",
    "    y = torch.linspace(-scale, scale, N)\n",
    "    z = torch.linspace(-scale, scale, N)\n",
    "    x, y, z = torch.meshgrid((x, y, z), indexing='ij')\n",
    "    xyz = torch.stack([x, y, z], dim=-1).reshape(-1, 3)\n",
    "    \n",
    "    # Process in batches\n",
    "    densities = []\n",
    "    colors = []\n",
    "    \n",
    "    for i in range(0, xyz.shape[0], batch_size):\n",
    "        batch_xyz = xyz[i:i+batch_size].to(device)\n",
    "        # Sample multiple directions for better color estimates\n",
    "        theta = torch.linspace(0, np.pi, 4)\n",
    "        phi = torch.linspace(0, 2*np.pi, 4)\n",
    "        theta, phi = torch.meshgrid((theta, phi), indexing='ij')\n",
    "        dirs = torch.stack([\n",
    "            torch.sin(theta) * torch.cos(phi),\n",
    "            torch.sin(theta) * torch.sin(phi),\n",
    "            torch.cos(theta)\n",
    "        ], dim=-1).view(-1, 3)\n",
    "        \n",
    "        batch_dirs = dirs.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_colors = []\n",
    "            for d in batch_dirs:\n",
    "                batch_d = d.expand(batch_xyz.shape[0], -1)\n",
    "                rgb, density = model.forward(batch_xyz, batch_d)\n",
    "                batch_colors.append(rgb)\n",
    "                \n",
    "                if len(batch_colors) == 1:  # Only need density once\n",
    "                    densities.append(density.cpu())\n",
    "            \n",
    "            # Average colors across viewing directions\n",
    "            avg_color = torch.stack(batch_colors).mean(0)\n",
    "            colors.append(avg_color.cpu())\n",
    "    \n",
    "    # Combine results\n",
    "    density = torch.cat(densities, dim=0).numpy().reshape(N, N, N)\n",
    "    colors = torch.cat(colors, dim=0)\n",
    "    \n",
    "    # Set threshold\n",
    "    if density_threshold is None:\n",
    "        density_threshold = 30 * np.mean(density)\n",
    "    \n",
    "    # Generate mesh\n",
    "    vertices, faces, normals, values = marching_cubes(\n",
    "        density,\n",
    "        level=density_threshold,\n",
    "        spacing=(scale*2/N, scale*2/N, scale*2/N)\n",
    "    )\n",
    "    \n",
    "    # Create mesh\n",
    "    mesh = trimesh.Trimesh(vertices=vertices, faces=faces, normals=normals)\n",
    "    \n",
    "    # Sample colors for vertices\n",
    "    vertex_colors = []\n",
    "    for vertex in vertices:\n",
    "        v_normalized = torch.tensor(vertex / (scale*2) * N).long()\n",
    "        v_normalized = torch.clamp(v_normalized, 0, N-1)\n",
    "        color = colors[v_normalized[0] * N*N + v_normalized[1] * N + v_normalized[2]]\n",
    "        vertex_colors.append(color.numpy())\n",
    "    \n",
    "    # Convert and assign colors\n",
    "    vertex_colors = np.array(vertex_colors)\n",
    "    vertex_colors = (vertex_colors * 255).astype(np.uint8)\n",
    "    mesh.visual.vertex_colors = vertex_colors\n",
    "    \n",
    "    # Clean up mesh\n",
    "    components = mesh.split(only_watertight=False)\n",
    "    if len(components) > 1:\n",
    "        areas = np.array([c.area for c in components])\n",
    "        mesh = components[np.argmax(areas)]\n",
    "    \n",
    "    mesh = trimesh.smoothing.filter_laplacian(mesh)\n",
    "    \n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac8b37-99bb-41b7-92e3-71ef9d9f9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "N = 250  # Resolution\n",
    "scale = 1.5\n",
    "batch_size = 50000\n",
    "density_threshold = None  # Will use mean-based threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c97ed5-c343-4e17-957a-137d1c1a1acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mesh with colors\n",
    "print(\"Generating colored mesh...\")\n",
    "mesh = generate_colored_mesh(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    N=N,\n",
    "    scale=scale,\n",
    "    batch_size=batch_size,\n",
    "    density_threshold=density_threshold\n",
    ")\n",
    "\n",
    "# Save mesh in different formats\n",
    "output_dir = \"nerf_output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as PLY (best for vertex colors)\n",
    "ply_path = os.path.join(output_dir, \"nerf_mesh_colored.ply\")\n",
    "mesh.invert()\n",
    "mesh.export(ply_path)\n",
    "print(f\"Saved colored mesh as PLY: {ply_path}\")\n",
    "\n",
    "# Save as OBJ with MTL\n",
    "obj_path = os.path.join(output_dir, \"nerf_mesh_colored.obj\")\n",
    "mesh.export(obj_path, include_texture=True)\n",
    "print(f\"Saved colored mesh as OBJ: {obj_path}\")\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nMesh Statistics:\")\n",
    "print(f\"Number of vertices: {len(mesh.vertices)}\")\n",
    "print(f\"Number of faces: {len(mesh.faces)}\")\n",
    "print(f\"Mesh volume: {mesh.volume:.2f}\")\n",
    "\n",
    "# Optional: Display the mesh\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de10ac4-4564-416c-81f5-0c6d22286d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newenv] *",
   "language": "python",
   "name": "conda-env-newenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
