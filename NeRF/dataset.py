import torch
import numpy as np
import os
import imageio
import json
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader


from colmap_utils import read_cameras_binary, read_images_binary, read_points3d_binary

def get_camera_poses(image_dir, colmap_dir):
    """
    Get camera poses either from COLMAP or initialize them if COLMAP failed
    
    Args:
        image_dir: Directory containing input images
        colmap_dir: Directory with COLMAP output
        
    Returns:
        dict with camera poses, intrinsics, and image paths
    """
    # Check if COLMAP results exist
    if os.path.exists(os.path.join(colmap_dir, "sparse/0")):
        try:
            return get_poses_from_colmap(colmap_dir)
        except Exception as e:
            print(f"Error parsing COLMAP output: {e}")
            print("Falling back to initialized poses")
    
    # Initialize poses in a circle if COLMAP failed
    return initialize_poses(image_dir)

def get_poses_from_colmap(colmap_dir):
    """Parse COLMAP output to get camera poses"""
    cameras = read_cameras_binary(os.path.join(colmap_dir, "sparse/0/cameras.bin"))
    images = read_images_binary(os.path.join(colmap_dir, "sparse/0/images.bin"))
    
    poses = []
    intrinsics = []
    img_paths = []
    
    for img_id in images:
        img = images[img_id]
        cam = cameras[img.camera_id]
        
        # Convert quaternion to rotation matrix
        R = qvec2rotmat(img.qvec)
        t = img.tvec
        
        # Create camera-to-world matrix
        w2c = np.zeros((4, 4), dtype=np.float32)
        w2c[:3, :3] = R
        w2c[:3, 3] = t
        w2c[3, 3] = 1
        
        c2w = np.linalg.inv(w2c)
        
        # Convert to NeRF convention
        c2w[:3, 1:3] *= -1  # Invert Y and Z
        
        poses.append(c2w)
        img_paths.append(img.name)
        
        # Extract focal length and principal point
        if cam.model == "SIMPLE_PINHOLE":
            focal_length = cam.params[0]
            cx, cy = cam.params[1], cam.params[2]
        elif cam.model == "PINHOLE":
            focal_length = (cam.params[0] + cam.params[1]) / 2
            cx, cy = cam.params[2], cam.params[3]
        else:
            focal_length = cam.params[0]
            cx, cy = cam.width / 2, cam.height / 2
            
        intrinsics.append([focal_length, focal_length, cx, cy])
    
    return {
        'poses': np.array(poses),
        'intrinsics': np.array(intrinsics),
        'img_paths': img_paths
    }

def initialize_poses(image_dir):
    """Initialize camera poses in a circle if COLMAP fails"""
    # Get list of images
    img_paths = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]
    
    # Get image dimensions from first image
    img = imageio.imread(os.path.join(image_dir, img_paths[0]))
    H, W = img.shape[:2]
    
    # Default focal length estimate
    focal = max(H, W)
    
    # Create poses in a circle
    poses = []
    intrinsics = []
    
    num_images = len(img_paths)
    for i in range(num_images):
        # Place cameras in a circle around the origin
        angle = 2 * np.pi * i / num_images
        radius = 4.0  # Distance from center
        
        # Camera position
        tx = radius * np.cos(angle)
        ty = 0.0  # Height
        tz = radius * np.sin(angle)
        
        # Camera viewing direction (looking at origin)
        camera_dir = np.array([-tx, -ty, -tz])
        camera_dir = camera_dir / np.linalg.norm(camera_dir)
        
        # Compute camera axes
        up = np.array([0.0, 1.0, 0.0])  # World up vector
        right = np.cross(camera_dir, up)
        right = right / np.linalg.norm(right)
        true_up = np.cross(right, camera_dir)
        
        # Create rotation matrix
        R = np.stack([right, true_up, -camera_dir], axis=1)  # Column-major
        
        # Create camera-to-world matrix
        c2w = np.zeros((4, 4), dtype=np.float32)
        c2w[:3, :3] = R
        c2w[:3, 3] = [tx, ty, tz]
        c2w[3, 3] = 1.0
        
        poses.append(c2w)
        intrinsics.append([focal, focal, W/2, H/2])
    
    return {
        'poses': np.array(poses),
        'intrinsics': np.array(intrinsics),
        'img_paths': img_paths
    }

def qvec2rotmat(qvec):
    """Convert quaternion to rotation matrix"""
    return np.array([
        [1 - 2 * qvec[2]**2 - 2 * qvec[3]**2, 2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3], 2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]],
        [2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3], 1 - 2 * qvec[1]**2 - 2 * qvec[3]**2, 2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]],
        [2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2], 2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1], 1 - 2 * qvec[1]**2 - 2 * qvec[2]**2]
    ])


def get_rays_from_transforms(datapath, mode='train'):
    """
    Load camera rays from a transforms.json file generated by colmap2nerf.py or run_glomap.py
    
    Args:
        datapath (str): Path to the dataset folder containing transforms.json
        mode (str): 'train' or 'test' to filter images
        
    Returns:
        rays_o (np.ndarray): Ray origins, shape (N, H*W, 3)
        rays_d (np.ndarray): Ray directions, shape (N, H*W, 3)
        target_px_values (np.ndarray): Target pixel values, shape (N, H*W, 3)
    """
    # Look for transforms.json in the current directory or in the datapath
    transforms_path = 'transforms.json'  # First try current directory
    if not os.path.exists(transforms_path):
        transforms_path = os.path.join(datapath, 'transforms.json')
    
    # Load the transforms.json file
    with open(transforms_path, 'r') as f:
        transforms = json.load(f)
    
    # Filter frames based on mode (train or test)
    frames = transforms['frames']
    
    # If we need to filter by train/test mode
    if mode in ['train', 'test']:
        # For compatibility with datasets that don't have train/test in filenames,
        # we'll use all frames if no frames match the filter
        filtered_frames = [f for f in frames if mode in f['file_path']]
        if filtered_frames:
            frames = filtered_frames
        else:
            print(f"No frames with '{mode}' in file_path found. Using all frames.")
    
    N = len(frames)
    
    if N == 0:
        raise ValueError(f"No frames found for mode '{mode}'. Check your transforms.json file.")
    
    # Get camera parameters either from root level or from first frame
    if 'w' in transforms and 'h' in transforms:
        # Parameters at root level
        H = int(transforms['h'])
        W = int(transforms['w'])
        
        # Try to find camera parameters at root level
        if 'fl_x' in transforms:
            focal_x = transforms['fl_x']
            focal_y = transforms.get('fl_y', focal_x)
            cx = transforms.get('cx', W / 2)
            cy = transforms.get('cy', H / 2)
        elif 'camera_angle_x' in transforms:
            # Calculate from camera angle
            camera_angle_x = transforms['camera_angle_x']
            focal_x = W / (2 * np.tan(camera_angle_x / 2))
            
            camera_angle_y = transforms.get('camera_angle_y', camera_angle_x)
            focal_y = H / (2 * np.tan(camera_angle_y / 2))
            
            cx = W / 2
            cy = H / 2
        else:
            # Default fallback
            focal_x = focal_y = max(H, W)
            cx = W / 2
            cy = H / 2
    else:
        # Try to get parameters from first frame
        first_frame = frames[0]
        
        if 'w' in first_frame and 'h' in first_frame:
            W = int(first_frame['w'])
            H = int(first_frame['h'])
        else:
            # Try to infer from the first image
            try:
                file_path = first_frame['file_path']
                if file_path.startswith('./'):
                    file_path = file_path[2:]
                
                img_path = os.path.join(datapath, file_path)
                if not os.path.exists(img_path):
                    img_path = file_path  # Try direct path
                
                img = imageio.imread(img_path)
                H, W = img.shape[:2]
                print(f"Inferred image dimensions from first frame: {W}x{H}")
            except Exception as e:
                print(f"Could not load first image to infer dimensions: {e}")
                W, H = 800, 800  # Default dimensions
        
        # Try to get camera parameters from first frame
        if 'fl_x' in first_frame:
            focal_x = first_frame['fl_x']
            focal_y = first_frame.get('fl_y', focal_x)
            cx = first_frame.get('cx', W / 2)
            cy = first_frame.get('cy', H / 2)
        elif 'camera_angle_x' in first_frame:
            # Calculate from camera angle
            camera_angle_x = first_frame['camera_angle_x']
            focal_x = W / (2 * np.tan(camera_angle_x / 2))
            
            camera_angle_y = first_frame.get('camera_angle_y', camera_angle_x)
            focal_y = H / (2 * np.tan(camera_angle_y / 2))
            
            cx = W / 2
            cy = H / 2
        else:
            # Default fallback
            focal_x = focal_y = max(H, W)
            cx = W / 2
            cy = H / 2
    
    print(f"Using camera parameters: W={W}, H={H}, focal_x={focal_x}, focal_y={focal_y}, cx={cx}, cy={cy}")
    
    # Initialize arrays - explicitly use float32
    rays_o = np.zeros((N, H*W, 3), dtype=np.float32)
    rays_d = np.zeros((N, H*W, 3), dtype=np.float32)
    images = []
    
    for i, frame in enumerate(frames):
        # Get transform matrix (camera-to-world)
        c2w = np.array(frame['transform_matrix'], dtype=np.float32)
        
        # Load image - handle either paths starting with ./ or not
        file_path = frame['file_path']
        if file_path.startswith('./'):
            file_path = file_path[2:]  # Remove ./ prefix
        
        img_path = file_path
        
        try:
            img = imageio.imread(img_path).astype(np.float32) / 255.0
        except FileNotFoundError:
            # Try looking in a few other places
            alternative_paths = [
                os.path.join(datapath, file_path),
                os.path.join(datapath, 'imgs', os.path.basename(file_path)),
                os.path.join('imgs', os.path.basename(file_path))
            ]
            
            for alt_path in alternative_paths:
                try:
                    img = imageio.imread(alt_path).astype(np.float32) / 255.0
                    print(f"Found image at {alt_path}")
                    break
                except FileNotFoundError:
                    continue
            else:
                raise FileNotFoundError(f"Could not find image at {file_path} or any alternative locations")
        
        # Handle RGBA images
        if img.shape[-1] == 4:
            img = img[..., :3] * img[..., -1:] + (1 - img[..., -1:])
        
        images.append(img[None, ...])
        
        # Generate rays
        u, v = np.meshgrid(np.arange(W), np.arange(H))
        # Convert pixel coordinates to camera space - using correct focal length scaling
        dirs = np.stack([(u - cx) / focal_x, -(v - cy) / focal_y, -np.ones_like(u)], axis=-1).astype(np.float32)
        # Rotate rays to world space
        dirs = (c2w[:3, :3] @ dirs.reshape(-1, 3, 1)).squeeze(-1)
        # Normalize ray directions
        dirs = dirs / np.linalg.norm(dirs, axis=-1, keepdims=True)
        
        rays_d[i] = dirs
        rays_o[i] = np.broadcast_to(c2w[:3, 3], (H*W, 3))
    
    # Concatenate images and reshape
    images = np.concatenate(images)
    target_px_values = images.reshape((N, H*W, 3))
    
    return rays_o, rays_d, target_px_values


def get_rays(datapath, mode='train'):
    """
    Main function to get rays and target pixel values.
    Automatically detects and uses transforms.json if it exists.
    
    Args:
        datapath (str): Path to the dataset folder
        mode (str): 'train' or 'test'
        
    Returns:
        rays_o (np.ndarray): Ray origins
        rays_d (np.ndarray): Ray directions
        target_px_values (np.ndarray): Target pixel values
    """
    # Always use transforms.json in this case
    return get_rays_from_transforms(datapath, mode)