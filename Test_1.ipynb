{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4e5d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/pytorch3d.git@stable\n",
      "  Cloning https://github.com/facebookresearch/pytorch3d.git (to revision stable) to /private/var/folders/wv/_x9hjmys03x5gnbfl70ry2sr0000gn/T/pip-req-build-b75njgs1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorch3d.git /private/var/folders/wv/_x9hjmys03x5gnbfl70ry2sr0000gn/T/pip-req-build-b75njgs1\n",
      "  Running command git checkout -q 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
      "  Resolved https://github.com/facebookresearch/pytorch3d.git to commit 75ebeeaea0908c5527e7b1e305fbc7681382db47\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: iopath in /Users/jorgemuyo/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pytorch3d==0.7.8) (0.1.10)\n",
      "Requirement already satisfied: tqdm in /Users/jorgemuyo/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from iopath->pytorch3d==0.7.8) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/jorgemuyo/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from iopath->pytorch3d==0.7.8) (4.11.0)\n",
      "Requirement already satisfied: portalocker in /Users/jorgemuyo/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from iopath->pytorch3d==0.7.8) (3.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Install PyTorch3D if needed\n",
    "if 'pytorch3d' not in sys.modules:\n",
    "    !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n",
    "\n",
    "from pytorch3d.transforms.so3 import so3_exp_map, so3_relative_angle\n",
    "from pytorch3d.renderer.cameras import SfMPerspectiveCameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e17d482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_dir):\n",
    "    \"\"\"Load images and compute SIFT features.\"\"\"\n",
    "    image_paths = list(Path(image_dir).glob('*.jpg')) + list(Path(image_dir).glob('*.png'))\n",
    "    images = []\n",
    "    features = []\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(str(path))\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = sift.detectAndCompute(gray, None)\n",
    "        images.append(img)\n",
    "        features.append((kp, des))\n",
    "        \n",
    "    return images, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2222e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relative_transforms(images, features):\n",
    "    \"\"\"Compute relative camera transforms using feature matching.\"\"\"\n",
    "    N = len(images)\n",
    "    edges = []\n",
    "    R_relative = []\n",
    "    T_relative = []\n",
    "    \n",
    "    # Camera intrinsic matrix (approximate)\n",
    "    K = np.array([\n",
    "        [1000, 0, images[0].shape[1]/2],\n",
    "        [0, 1000, images[0].shape[0]/2],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    matcher = cv2.BFMatcher()\n",
    "    \n",
    "    for i in range(N):\n",
    "        kp1, des1 = features[i]\n",
    "        for j in range(i+1, N):\n",
    "            kp2, des2 = features[j]\n",
    "            \n",
    "            # Match features\n",
    "            matches = matcher.knnMatch(des1, des2, k=2)\n",
    "            good_matches = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "                    \n",
    "            if len(good_matches) > 20:\n",
    "                # Get matching points\n",
    "                pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n",
    "                pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n",
    "                \n",
    "                # Compute essential matrix and recover pose\n",
    "                E, mask = cv2.findEssentialMat(pts1, pts2, K)\n",
    "                _, R, t, _ = cv2.recoverPose(E, pts1, pts2, K)\n",
    "                \n",
    "                edges.append([i, j])\n",
    "                R_relative.append(torch.from_numpy(R.astype(np.float32)))\n",
    "                T_relative.append(torch.from_numpy(t[:, 0].astype(np.float32)))\n",
    "    \n",
    "    return torch.stack(R_relative), torch.stack(T_relative), torch.tensor(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d131abfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_camera_positions(R_relative, T_relative, relative_edges, n_cameras, device='cpu'):\n",
    "    \"\"\"Optimize absolute camera positions given relative transforms.\"\"\"\n",
    "    log_R_absolute = torch.randn(n_cameras, 3, device=device)\n",
    "    T_absolute = torch.randn(n_cameras, 3, device=device)\n",
    "    \n",
    "    # First camera at origin\n",
    "    log_R_absolute[0, :] = 0.\n",
    "    T_absolute[0, :] = 0.\n",
    "    \n",
    "    log_R_absolute.requires_grad = True\n",
    "    T_absolute.requires_grad = True\n",
    "    \n",
    "    cameras_relative = SfMPerspectiveCameras(\n",
    "        R=R_relative.to(device),\n",
    "        T=T_relative.to(device),\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam([log_R_absolute, T_absolute], lr=0.01)\n",
    "    \n",
    "    for iteration in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        R_absolute = so3_exp_map(log_R_absolute)\n",
    "        \n",
    "        cameras_absolute = SfMPerspectiveCameras(\n",
    "            R=R_absolute,\n",
    "            T=T_absolute,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        trans_i = cameras_absolute.get_world_to_view_transform()[relative_edges[:, 0]]\n",
    "        trans_j = cameras_absolute.get_world_to_view_transform()[relative_edges[:, 1]]\n",
    "        trans_rel = trans_i.inverse().compose(trans_j)\n",
    "        \n",
    "        matrix_rel = trans_rel.get_matrix()\n",
    "        R_composed = matrix_rel[:, :3, :3]\n",
    "        T_composed = matrix_rel[:, 3, :3]\n",
    "        \n",
    "        R_loss = (1. - so3_relative_angle(R_composed, cameras_relative.R, cos_angle=True)).mean()\n",
    "        T_loss = ((T_composed - cameras_relative.T)**2).sum(1).mean()\n",
    "        \n",
    "        loss = R_loss + T_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f'Iteration {iteration}, Loss: {loss.item():.6f}')\n",
    "    \n",
    "    return R_absolute.detach(), T_absolute.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load and process images\n",
    "images = load_images('media')\n",
    "images = images.to(device)\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    features = extract_features(images)\n",
    "\n",
    "# Compute relative transforms\n",
    "R_relative, T_relative, relative_edges = compute_relative_transforms(features)\n",
    "\n",
    "# Optimize camera positions\n",
    "R_absolute, T_absolute = optimize_camera_positions(\n",
    "    R_relative, \n",
    "    T_relative, \n",
    "    relative_edges, \n",
    "    n_cameras=len(images),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for i in range(len(images)):\n",
    "    print(f'\\nCamera {i}:')\n",
    "    print(f'Rotation:\\n{R_absolute[i]}')\n",
    "    print(f'Translation: {T_absolute[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a02de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1c5f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c177d57c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:newenv] *",
   "language": "python",
   "name": "conda-env-newenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
